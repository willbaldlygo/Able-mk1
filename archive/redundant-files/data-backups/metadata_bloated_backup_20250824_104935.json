{
  "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63": {
    "id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
    "name": "Stanford_Research_Tool.pdf",
    "file_path": "/Users/will/AVI BUILD/Able3_Main_WithVoiceMode/data/sources/Stanford_Research_Tool.pdf",
    "summary": "Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models Yijia Shao Yucheng Jiang Theodore A. Kanell Peter Xu Omar Khattab Monica S. Lam Stanford University {shaoyj, yuchengj, tkanell, peterxu, okhattab}@stanford.edu lam@cs.stanford.edu Abstract We study how to apply l...",
    "chunks": [
      {
        "id": "4295de55-a8ed-432b-a8fe-d9e5795cbb77",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models Yijia Shao Yucheng Jiang Theodore A. Kanell Peter Xu Omar Khattab Monica S. Lam Stanford University {shaoyj, yuchengj, tkanell, peterxu, okhattab}@stanford.edu lam@cs.stanford.edu Abstract We study how to apply large language models to write grounded and organized long-form ar- ticles from scratch, with comparable breadth and depth to Wikipedia pages. This underex- plored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writ- ing. We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Ask- ing. STORM models the pre-writing stage by (1) discovering diverse perspectives in research- ing the given topic, (2) simulating conversa- tions where writers carrying different perspec- tives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the col- lected information to create an outline. For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from experienced Wikipedia editors. Com- pared to articles generated by an outline- driven retrieval-augmented baseline, more of STORM\u2019s articles are deemed to be organized (by a 25% absolute increase) and broad in cov- erage (by 10%). The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts. 1 Introduction Large language models (LLMs) have demonstrated impressive writing capabilities (Yang et al., 2023; Pavlik, 2023; Wenzlaff and Spaeth, 2022; Fitria, 2023), but it is unclear how we can use them to write grounded, long-form articles, like full-length Wikipedia pages. Such expository writing, which seeks to inform the reader on a topic in an or- ganized manner (Weaver III and Kintsch, 1991; Balepur et al., 2023), requires thorough research and planning in the pre-writing stage (Rohman, (A) Direct Prompting Prompt: Ask 30 questions about the given topic. 1. When was the opening ceremony held? 2. Where was the opening ceremony held? 3. How many countries participated in the opening ceremony? ... (B) Perspective-Guided Question Asking Prompt: You are an event planner who focuses on the preparation of the opening ceremony. \u2026 (C) Conversational Question Asking Can you provide me with a list of the participating countries in the 2022 Winter Olympics opening ceremony? The 2022 Winter Olympics featured a diverse group of countries participating in the opening ceremony. These included \u2026 Athletes from over 90 countries will enter the stadium in a specific order. How is the order of participating countries in the 2022 Winter Olympics opening ceremony determined? 1. Can you provide any information about the transportation arrangements for the opening ceremony? 2. Can you provide any information about the budget for the 2022 Winter Olympics opening ceremony? \u2026 LLM LLM LLM- Role1 LLM- Role2 LLM- Role1 Topic References Outline Full-length Article Research via Question Asking 2022 Winter Olympics Opening Ceremony Prewriting Writing Figure 1: We explore writing Wikipedia-like articles from scratch, which demands a pre-writing stage before producing the article. In this stage, simpler approaches like Direct Prompting have limited planning capacity. In contrast, STORM researches the topic via perspective- guided question asking in simulated conversations. 1965), even before the actual writing process can start. However, prior work on generating Wikipedia articles (Banerjee and Mitra, 2015; Minguill\u00f3n et al., 2017; Liu et al., 2018; Fan and Gardent, 2022) has generally bypassed the pre-writing stage: for instance, Liu et al. (2018) presume reference documents are provided in advance, while Fan and Gardent (2022) assume an article outline is avail- able and focus on",
        "chunk_index": 0,
        "start_char": 0,
        "end_char": 4031
      },
      {
        "id": "a658cb0a-3805-477a-afe2-4862ca12c345",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "demands a pre-writing stage before producing the article. In this stage, simpler approaches like Direct Prompting have limited planning capacity. In contrast, STORM researches the topic via perspective- guided question asking in simulated conversations. 1965), even before the actual writing process can start. However, prior work on generating Wikipedia articles (Banerjee and Mitra, 2015; Minguill\u00f3n et al., 2017; Liu et al., 2018; Fan and Gardent, 2022) has generally bypassed the pre-writing stage: for instance, Liu et al. (2018) presume reference documents are provided in advance, while Fan and Gardent (2022) assume an article outline is avail- able and focus on expanding each section. These assumptions do not hold in general, as collecting references and crafting outlines demand advanced information literacy skills (Doyle, 1994) to iden- arXiv:2402.14207v2 [cs.CL] 8 Apr 2024 tify, evaluate, and organize external sources - a task that is challenging even for experienced writers. Automating this process can facilitate individuals in initiating in-depth learning about a topic and greatly reduce the expensive expert hours neces- sary for their expository writing. We explore these challenges by focusing on how to generate Wikipedia-like articles from scratch. We decompose this problem into two tasks. The first is to conduct research to generate an outline, i.e., a list of multi-level sections, and collect a set of reference documents. The second uses the outline and the references to produce the full-length arti- cle. Such a task decomposition mirrors the human writing process which usually includes phases of pre-writing, drafting, and revising (Rohman, 1965; Munoz-Luna, 2015). As pre-trained language models inherently pos- sess a wealth of knowledge, a direct approach is to rely on their parametric knowledge for generating outlines or even entire articles (Direct Gen). How- ever, this approach is limited by a lack of details and hallucinations (Xu et al., 2023), particularly in addressing long-tail topics (Kandpal et al., 2023). This underscores the importance of leveraging ex- ternal sources, and current strategies often involve retrieval-augmented generation (RAG), which cir- cles back to the problem of researching the topic in the pre-writing stage, as much information cannot be surfaced through simple topic searches. Human learning theories (Tawfik et al., 2020; Booth et al., 2003) highlight asking effective questions in information acquisition. Although instruction-tuned models (Ouyang et al., 2022) can be prompted directly to generate questions, we find that they typically produce basic \u201cWhat\u201d, \u201cWhen\u201d, and \u201cWhere\u201d questions (Figure 1 (A)) which often only address surface-level facts about the topic. To endow LLMs with the capacity to conduct better research, we propose the STORM paradigm for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. The design of STORM is based on two hypothe- ses: (1) diverse perspectives lead to varied ques- tions; (2) formulating in-depth questions requires iterative research. Building upon these hypotheses, STORM employs a novel multi-stage approach. It first discovers diverse perspectives by retrieving and analyzing Wikipedia articles from similar top- ics and then personifies the LLM with specific per- spectives for question asking (Figure 1 (B)). Next, to elicit follow-up questions for iterative research (Figure 1 (C)), STORM simulates multi-turn con- versations where the answers to the generated ques- tions are grounded on the Internet. Finally, based on the LLM\u2019s internal knowledge and the collected information, STORM creates an outline that can be expanded section by section to develop a full- length Wikipedia-like article. We evaluate STORM using our FreshWiki dataset (\u00a72.1) which curates recent, high-quality Wikipedia articles to avoid data leakage during pre- training.1 To facilitate the study of the pre-writing stage, we define metrics for evaluating the outline quality against human-written articles. We further invited a",
        "chunk_index": 1,
        "start_char": 3361,
        "end_char": 7437
      },
      {
        "id": "a148b236-e4ce-4047-90f8-084943bf2292",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "elicit follow-up questions for iterative research (Figure 1 (C)), STORM simulates multi-turn con- versations where the answers to the generated ques- tions are grounded on the Internet. Finally, based on the LLM\u2019s internal knowledge and the collected information, STORM creates an outline that can be expanded section by section to develop a full- length Wikipedia-like article. We evaluate STORM using our FreshWiki dataset (\u00a72.1) which curates recent, high-quality Wikipedia articles to avoid data leakage during pre- training.1 To facilitate the study of the pre-writing stage, we define metrics for evaluating the outline quality against human-written articles. We further invited a group of experienced Wikipedia editors for expert evaluation. The ed- itors found STORM outperforms an outline-driven RAG baseline, especially regarding the breadth and organization of the articles. They also identified challenges for future research, including address- ing cases where: (1) the bias on the Internet affects the generated articles; (2) LLMs fabricate connec- tions between unrelated facts. These challenges present new frontiers to grounded writing systems. Our main contributions include: \u2022 To evaluate the capacity of LLM systems at generating long-form grounded articles from scratch, and the pre-writing challenge in par- ticular, we curate the FreshWiki dataset and establish evaluation criteria for both outline and final article quality. \u2022 We propose STORM, a novel system that au- tomates the pre-writing stage. STORM re- searches the topic and creates an outline by using LLMs to ask incisive questions and re- trieving trusted information from the Internet. \u2022 Both automatic and human evaluation demon- strate the effectiveness of our approach. Ex- pert feedback further reveals new challenges in generating grounded long-form articles. 2 FreshWiki We study generating Wikipedia-like articles from scratch, placing emphasis on the pre-writing stage (Rohman, 1965), which involves the demand- ing sub-tasks of gathering and curating relevant information (\u201cresearch\u201d). This models the human 1Our resources and code are released at https://github. com/stanford-oval/storm. Domain Scope Given Outline? Given Refs? Balepur et al. (2023) One One para. / Yes Qian et al. (2023) All One para. / No Fan and Gardent (2022) One Full article Yes No Liu et al. (2018) All One para. / Yes Sauper and Barzilay (2009) Two Full article No No Ours All Full article No No Table 1: Comparison of different Wikipedia generation setups in existing literature. Generating one paragraph does not need an article outline. writing approach which has prompted some educa- tors to view Wikipedia article writing as an educa- tional exercise for academic training (Tardy, 2010). Table 1 compares our work against prior bench- marks for Wikipedia generation. Existing work has generally focused on evaluating the generation of shorter snippets (e.g., one paragraph), within a narrower scope (e.g., a specific domain or two), or when an explicit outline or reference documents are supplied. A notable example is WikiSum (Liu et al., 2018), which treats generating Wikipedia ar- ticles as a multi-document summarization problem, with respect to the reference documents. Our setup emphasizes the capability of long- form grounded writing systems to research and curate content. Specifically, given a topic t, the task is to find a set of references R and generate a full-length article S = s1s2...sn, where each sentence si cites a list of documents in R.2 2.1 The FreshWiki Dataset Creating a new Wikipedia-like article demands not only fluent writing but also good research skills. As modern LLMs are generally trained on Wikipedia text, we mitigate data leakage by explicitly seeking out recent Wikipedia articles that were created (or very heavily edited) after the training cutoff of the LLMs we test. Our process can be repeated at future dates when",
        "chunk_index": 2,
        "start_char": 6751,
        "end_char": 10687
      },
      {
        "id": "b01a4ba5-9b27-4b89-9366-10bfc2655673",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "curate content. Specifically, given a topic t, the task is to find a set of references R and generate a full-length article S = s1s2...sn, where each sentence si cites a list of documents in R.2 2.1 The FreshWiki Dataset Creating a new Wikipedia-like article demands not only fluent writing but also good research skills. As modern LLMs are generally trained on Wikipedia text, we mitigate data leakage by explicitly seeking out recent Wikipedia articles that were created (or very heavily edited) after the training cutoff of the LLMs we test. Our process can be repeated at future dates when new LLMs emerge. To apply our date criteria, we focus on the top 100 most-edited pages, based on edit counts, for each month from February 2022 to September 20233. To ensure high-quality references, we filter these articles to keep only those having B-class quality or above assessed by ORES4. We also ex- 2In practice, S also includes organizational elements such as section and subsection titles, which do not require citations. 3Obtained from https://wikimedia. org/api/rest_v1/metrics/edited-pages/ top-by-edits/en.wikipedia/all-editor-types/ content/{year}/{month}/all-days 4https://www.mediawiki.org/wiki/ORES clude list articles5 and articles that have no sub- sections. While high-quality Wikipedia articles usually contain structured data (e.g., tables) and are multi-modal, we only consider the plain text com- ponent in constructing the dataset to simplify our task. More details of the dataset are in Appendix A. 2.2 Outline Creation and Evaluation A full-length article is hard to generate or evalu- ate (Xu et al., 2023; Krishna et al., 2023). When human educators teach students academic writing, they sometimes supervise students at the outline stage (Eriksson and M\u00e4kitalo, 2015) because an extensive outline indicates a comprehensive under- standing of the topic and provides a solid founda- tion for writing the full-length article (Dietz and Foley, 2019). Inspired by this, we decompose the generation of S into two stages. In the pre-writing stage, we require the system to create an outline O, which is defined as a list of multi-level section headings6. In the writing stage, the system uses the topic t, the references R, and an outline O to produce the full-length article S. To evaluate the outline coverage, we introduce two metrics: heading soft recall and heading en- tity recall. These metrics compare the multi-level section headings of the human-written article, con- sidered as ground truth, and those in O. Recog- nizing that an exact match between elements in these two sets of headings is unnecessary, we cal- culate the heading soft recall (Fr\u00e4nti and Mariescu- Istodor, 2023) using cosine similarity derived from Sentence-BERT (Reimers and Gurevych, 2019) em- beddings of the headings (details in Appendix C.1). We also compute the heading entity recall which is quantified as the percentage of named entities in human-written article headings covered by O. We extract entities with FLAIR named entity recogni- tion (NER) (Akbik et al., 2019). 3 Method We present STORM to automate the pre-writing stage by researching a given topic via effective question asking (\u00a73.1, \u00a73.2) and creating an out- line (\u00a73.3). The outline will be extended to a full- length article grounded on the collected references 5https://en.wikipedia.org/wiki/Wikipedia: Stand-alone_lists 6Since language models process and produce sequences, we can linearize O by adding \u201c#\u201d to indicate section titles, \u201c##\u201d to indicate subsection titles, etc. Related Articles Topic \ud835\udc95 \u2460 Survey \u2461 Identify Perspectives Draft Outline \ud835\udcaa! \u2466 Direct Generate References \u211b Question \ud835\udc92 Answer \ud835\udc82 Wikipedia Writer \u2462 Read & Ask Expert \u2463 Split Queries \u2464 Search & Sift \u2465 Synthesize Add Specific Perspective Outline \ud835\udcaa Gather Conversations {\ud835\udc9e\", \u2026 , \ud835\udc9e#} \u2467 Refine \ud835\udcab Add Trusted Sources Figure 2: The overview of",
        "chunk_index": 3,
        "start_char": 10094,
        "end_char": 13977
      },
      {
        "id": "2994da0a-fe00-47d0-be6a-14c9e359f0b1",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "The outline will be extended to a full- length article grounded on the collected references 5https://en.wikipedia.org/wiki/Wikipedia: Stand-alone_lists 6Since language models process and produce sequences, we can linearize O by adding \u201c#\u201d to indicate section titles, \u201c##\u201d to indicate subsection titles, etc. Related Articles Topic \ud835\udc95 \u2460 Survey \u2461 Identify Perspectives Draft Outline \ud835\udcaa! \u2466 Direct Generate References \u211b Question \ud835\udc92 Answer \ud835\udc82 Wikipedia Writer \u2462 Read & Ask Expert \u2463 Split Queries \u2464 Search & Sift \u2465 Synthesize Add Specific Perspective Outline \ud835\udcaa Gather Conversations {\ud835\udc9e\", \u2026 , \ud835\udc9e#} \u2467 Refine \ud835\udcab Add Trusted Sources Figure 2: The overview of STORM that automates the pre-writing stage. Starting with a given topic, STORM identifies various perspectives on covering the topic by surveying related Wikipedia articles ( 1 - 2 ). It then simulates conversations between a Wikipedia writer who asks questions guided by the given perspective and an expert grounded on trustworthy online sources ( 3 - 6 ). The final outline is curated based on the LLM\u2019s intrinsic knowledge and the gathered conversations from different perspectives ( 7 - 8 ). (\u00a73.4). Figure 2 gives an overview of STORM and we include the pseudo code in Appendix B. 3.1 Perspective-Guided Question Asking Rohman (1965) defines pre-writing as the stage of discovery in the writing process. In parallel with stakeholder theory in business (Freeman et al., 2010), where diverse stakeholders prioritize vary- ing facets of a company, individuals with distinct perspectives may concentrate on different aspects when researching the same topic and discover mul- tifaceted information. Further, the specific perspec- tives can serve as prior knowledge, guiding individ- uals to ask more in-depth questions. For example, an event planner might ask about the \u201ctransporta- tion arrangements\u201d and \u201cbudget\u201d for \u201cthe 2022 Winter Olympics opening ceremony\u201d, whereas a layperson might ask more general questions about the event\u2019s basic information (Figure 1 (A)). Given the input topic t, STORM discovers differ- ent perspectives by surveying existing articles from similar topics and uses these perspectives to control the question asking process. Specifically, STORM prompts an LLM to generate a list of related top- ics and subsequently extracts the tables of contents from their corresponding Wikipedia articles, if such articles can be obtained through Wikipedia API7 (Figure 2 1 ). These tables of contents are con- catenated to create a context to prompt the LLM to identify N perspectives P = {p1, ..., pN} that 7https://pypi.org/project/Wikipedia-API/ can collectively contribute to a comprehensive ar- ticle on t (Figure 2 2 ). To ensure that the basic information about t is also covered, we add p0 as \u201cbasic fact writer focusing on broadly covering the basic facts about the topic\u201d into P. Each perspec- tive p \u2208P will be utilized to guide the LLM in the process of question asking in parallel. 3.2 Simulating Conversations The theory of questions and question asking (Ram, 1991) highlights that while answers to existing questions contribute to a more comprehensive understanding of a topic, they often simultane- ously give rise to new questions. To kick off this dynamic process, STORM simulates a conversa- tion between a Wikipedia writer and a topic ex- pert. In the i-th round of the conversation, the LLM-powered Wikipedia writer generates a sin- gle question qi based on the topic t, its assigned perspective p \u2208P, and the conversation history {q1, a1, ..., qi\u22121, ai\u22121} where aj denotes the sim- ulated expert\u2019s answer. The conversation history enables the LLM to update its understanding of the topic and ask follow-up questions. In practice, we limit the conversation to at most M rounds. To ensure that the conversation history",
        "chunk_index": 4,
        "start_char": 13336,
        "end_char": 17133
      },
      {
        "id": "67d1d7ee-60f7-4cf7-8aad-e325bc9a6062",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "new questions. To kick off this dynamic process, STORM simulates a conversa- tion between a Wikipedia writer and a topic ex- pert. In the i-th round of the conversation, the LLM-powered Wikipedia writer generates a sin- gle question qi based on the topic t, its assigned perspective p \u2208P, and the conversation history {q1, a1, ..., qi\u22121, ai\u22121} where aj denotes the sim- ulated expert\u2019s answer. The conversation history enables the LLM to update its understanding of the topic and ask follow-up questions. In practice, we limit the conversation to at most M rounds. To ensure that the conversation history provides factual information, we use trusted sources from the Internet to ground the answer ai to each query qi. Since qi can be complicated, we first prompt the LLM to break down qi into a set of search queries (Figure 2 4 ) and the searched results will be evaluated using a rule-based filter according to the Wikipedia guideline8 to exclude untrustworthy sources (Figure 2 5 ). Finally, the LLM synthe- sizes the trustworthy sources to generate the answer ai, and these sources will also be added to R for full article generation (\u00a73.4). 3.3 Creating the Article Outline After thoroughly researching the topic through N + 1 simulated conversations, denoted as {C0, C1, ..., CN}, STORM creates an outline before the actual writing starts. To fully leverage the inter- nal knowledge of LLMs, we first prompt the model to generate a draft outline OD given only the topic t (Figure 2 7 ). OD typically provides a general but organized framework. Subsequently, the LLM is prompted with the topic t, the draft outline OD, and the simulated conversations {C0, C1, ..., CN} to refine the outline (Figure 2 8 ). This results in an improved outline O which will be used for producing the full-length article. 3.4 Writing the Full-Length Article Building upon the references R collected and the outline O developed during the pre-writing stage, the full-length article can be composed section by section. Since it is usually impossible to fit the entire R within the context window of the LLM, we use the section title and headings of its all-level subsections to retrieve relevant documents from R based on semantic similarity calculated from Sentence-BERT embeddings. With the relevant in- formation at hand, the LLM is then prompted to generate the section with citations. Once all sec- tions are generated, they are concatenated to form the full-length article. Since the sections are gen- erated in parallel, we prompt the LLM with the concatenated article to delete repeated information to improve coherence. Furthermore, in alignment with Wikipedia\u2019s stylistic norms, the LLM is also utilized to synthesize a summary of the entire arti- cle, forming the lead section at the beginning. 4 Experiments 4.1 Article Selection STORM is capable of researching complicated top- ics and writing long articles from detailed outlines. However, in this controlled experiment, we limit the final output to at most 4000 tokens (roughly 3000 words). For a meaningful comparison, we 8https://en.wikipedia.org/wiki/Wikipedia: Reliable_sources randomly select 100 samples from the FreshWiki dataset (see \u00a72.1) that have human-written articles not exceeding 3000 words. 4.2 Automatic Metrics As discussed in \u00a72.2, we evaluate the outline qual- ity to assess the pre-writing stage by calculating the heading soft recall and heading entity recall. A higher recall score signifies a more comprehensive outline relative to the human-written article. To assess the full-length article quality, we adopt ROUGE scores (Lin, 2004) and compute the entity recall in the article level based on FLAIR NER results. Moreover, based on Wikipedia criteria9, we evaluate the",
        "chunk_index": 5,
        "start_char": 16529,
        "end_char": 20271
      },
      {
        "id": "74faa17e-acf1-4875-ad03-94bfa55a871a",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "a meaningful comparison, we 8https://en.wikipedia.org/wiki/Wikipedia: Reliable_sources randomly select 100 samples from the FreshWiki dataset (see \u00a72.1) that have human-written articles not exceeding 3000 words. 4.2 Automatic Metrics As discussed in \u00a72.2, we evaluate the outline qual- ity to assess the pre-writing stage by calculating the heading soft recall and heading entity recall. A higher recall score signifies a more comprehensive outline relative to the human-written article. To assess the full-length article quality, we adopt ROUGE scores (Lin, 2004) and compute the entity recall in the article level based on FLAIR NER results. Moreover, based on Wikipedia criteria9, we evaluate the article from the aspects of (1) In- terest Level, (2) Coherence and Organization, (3) Relevance and Focus, (4) Coverage, and (5) Verifia- bility. For aspects (1)-(4), we use Prometheus (Kim et al., 2023), a 13B evaluator LLM to score the arti- cle based on a 5-point rubric collaboratively devel- oped with two experienced Wikipedia editors (see Appendix C.2). For verifiability, we calculate the citation recall and citation precision based on the definition in Gao et al. (2023). We use Mistral 7B- Instruct (Jiang et al., 2023a) to examine whether the cited passages entail the generated sentence. 4.3 Baselines As prior works use different setups and do not use LLMs, they are hard to compare directly. Instead, we use the following three LLM-based baselines. 1. Direct Gen, a baseline that directly prompts the LLM to generate an outline, which is then used to generate the full-length article. 2. RAG, a retrieval-augmented generation base- line that searches with the topic and uses the searched results together with the topic t to generate an outline or the entire article. 3. Outline-driven RAG (oRAG), which is iden- tical to RAG in outline creation, but further searches additional information with section titles to generate the article section by section. 4.4 STORM Implementation We build STORM with zero-shot prompting us- ing the DSPy framework (Khattab et al., 2023). Appendix B includes the pseudo code and corre- sponding prompts. The hyperparameters N and M 9https://en.wikipedia.org/wiki/Wikipedia: Good_article_criteria Comparsion with Human-written Articles Rubric Grading ROUGE-1 ROUGE-L Entity Recall Interest Level Organization Relevance Coverage Direct Gen 25.62 12.63 5.08 2.87 4.60 3.10 4.16 RAG 28.52 13.18 7.57 3.14 4.22 3.05 4.08 oRAG 44.26 16.51 12.57 3.90 4.79 4.09 4.70 STORM 45.82 16.70 14.10\u2020 3.99\u2020 4.82 4.45\u2020 4.88\u2020 w/o Outline Stage 26.77 12.77 7.39 3.33 4.87 3.35 4.37 Table 2: Results of automatic article quality evaluation. \u2020 denotes significant differences (p < 0.05) from a paired t-test between STORM and the best baseline, i.e., oRAG. The rubric grading uses a 1-5 scale. Heading Soft Recall Heading Entity Recall GPT-3.5 Direct Gen 80.23 32.39 RAG/oRAG 73.59 33.85 RAG-expand 74.40 33.85 STORM 86.26\u2020 40.52\u2020 w/o Perspective 84.49 40.12 w/o Conversation 77.97 31.98 GPT-4 Direct Gen 87.66 34.78 RAG/oRAG 89.55 42.38 RAG-expand 91.36 43.53 STORM 92.73\u2020 45.91 w/o Perspective 92.39 42.70 w/o Conversation 88.75 39.30 Table 3: Results of outline quality evaluation (%). \u2020 de- notes significant differences (p < 0.05) from a paired t-test between STORM and baselines. in STORM are both set as 5. We use the chat model gpt-3.5-turbo for question asking and use gpt-3.5-turbo-instruct for other parts of STORM. We also experiment with using gpt-4 for drafting and refining the outline (Figure 2 7 - 8 ). For reported results, the simulated topic expert in STORM is grounded on the You.com search API10, although the proposed pipeline is compatible with other search engines. The ground truth Wikipedia article is excluded from the search results. For final article generation, we only report the results using gpt-4 as gpt-3.5 is not faithful",
        "chunk_index": 6,
        "start_char": 19572,
        "end_char": 23455
      },
      {
        "id": "123d144e-dfe6-46a7-96dd-f867c451304d",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "t-test between STORM and baselines. in STORM are both set as 5. We use the chat model gpt-3.5-turbo for question asking and use gpt-3.5-turbo-instruct for other parts of STORM. We also experiment with using gpt-4 for drafting and refining the outline (Figure 2 7 - 8 ). For reported results, the simulated topic expert in STORM is grounded on the You.com search API10, although the proposed pipeline is compatible with other search engines. The ground truth Wikipedia article is excluded from the search results. For final article generation, we only report the results using gpt-4 as gpt-3.5 is not faithful to sources when generating text with citations (Gao et al., 2023). We set temperature as 1.0 and top_p as 0.9 for all experiments. 5 Results and Analysis 5.1 Main Results We use outline coverage as a proxy to assess the pre- writing stage (see \u00a72.2). Table 3 shows the heading soft recall and entity recall. Outlines directly gen- erated by LLMs (Direct Gen) already demonstrate 10https://documentation.you.com/api-reference/ search high heading soft recall, indicating LLMs\u2019 ability to grasp high-level aspects of a topic through their rich parametric knowledge. However, STORM, by asking effective questions to research the topic, can create higher recall outlines that cover more topic- specific aspects. Notably, although RAG leverages additional information, presenting unorganized in- formation in the context window makes outline generation more challenging for the weaker model, i.e., GPT-3.5, leading to worse performance. To test the limit of the RAG baseline, we further expand the retrieved sources by starting with the outline produced by RAG, using its section titles as search queries to collect more sources, and inputting the newly collected sources together with the initial outline to LLM to generate a polished outline. This modified approach is referred to as \u201cRAG-expand\u201d in Table 3. The experiment results indicate that even though having an additional round of search and refinement can improve the outline produced by RAG, our proposed STORM still surpasses its performance. We further evaluate the full-length article quality. As shown in Table 2, oRAG significantly outper- forms RAG, highlighting the effectiveness of using outlines for structuring full-length article genera- tion. Despite this method\u2019s advantages in leverag- ing retrieval and outlining, our approach still out- performs it. The effective question asking mecha- nism enhances the articles with greater entity recall. The evaluator LLM also rates these articles with sig- nificantly higher scores in the aspects of \u201cInterest Level\u201d, \u201cRelevance and Focus\u201d, and \u201cCoverage\u201d. Nonetheless, we acknowledge the possibility of the evaluator LLM overrating machine-generated text. Our careful human evaluation (\u00a76) reveals that STORM still has much room for improvement. Although this work primarily focuses on the pre- writing stage and does not optimize generating text with citations, we still examine the citation quality of articles produced by our approach. As reported Citation Recall Citation Precision STORM 84.83 85.18 Table 4: Citation quality judged by Mistral 7B-Instruct. STORM w/o Perspective w/o Conversation |R| 99.83 54.36 39.56 Table 5: Average number of unique references (|R|) collected using different methods. in Table 4, Mistral 7B-Instruct judges 84.83% of the sentences are supported by their citations. Ap- pendix C.3 investigates the unsupported sentences and reveals that the primary issues stem from draw- ing improper inferences and inaccurate paraphras- ing, rather than hallucinating non-existent contents. 5.2 Ablation Studies As introduced in \u00a73, STORM prompts LLMs to ask effective questions by discovering specific perspectives and simulating multi-turn conversa- tions. We conduct the ablation study on outline creation by comparing STORM with two variants: (1) \u201cSTORM w/o Perspective\u201d, which omits per- spective in the question generation prompt; (2) \u201cSTORM w/o Conversation\u201d, which",
        "chunk_index": 7,
        "start_char": 22847,
        "end_char": 26864
      },
      {
        "id": "24085c2e-1ccc-46bd-8c81-6e78a347385a",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "different methods. in Table 4, Mistral 7B-Instruct judges 84.83% of the sentences are supported by their citations. Ap- pendix C.3 investigates the unsupported sentences and reveals that the primary issues stem from draw- ing improper inferences and inaccurate paraphras- ing, rather than hallucinating non-existent contents. 5.2 Ablation Studies As introduced in \u00a73, STORM prompts LLMs to ask effective questions by discovering specific perspectives and simulating multi-turn conversa- tions. We conduct the ablation study on outline creation by comparing STORM with two variants: (1) \u201cSTORM w/o Perspective\u201d, which omits per- spective in the question generation prompt; (2) \u201cSTORM w/o Conversation\u201d, which prompts LLMs to generate a set number of questions altogether. To ensure a fair comparison, we control an equal total number of generated questions across all variants. Table 3 shows the ablation results and full STORM pipeline produces outlines with the highest recall. Also, \u201cSTORM w/o Conversation\u201d gives much worse results, indicating reading relevant informa- tion is crucial to generating effective questions. We further examine how many unique sources are col- lected in R via different variants. As shown in Ta- ble 5, the full pipeline discovers more different sources and the trend is in accord with the auto- matic metrics for outline quality. We also verify whether having an outline stage is necessary with STORM. In Table 2, \u201cSTORM w/o Outline Stage\u201d denotes the results of generat- ing the entire article given the topic and the sim- ulated conversations. Removing the outline stage significantly deteriorates the performance across all metrics. 6 Human Evaluation To better understand the strengths and weaknesses of STORM, we conduct human evaluation by col- laborating with 10 experienced Wikipedia editors oRAG STORM p-value Avg. \u22654 Rates Av.g. \u22654 Rates Interest Level 3.63 57.5% 4.03 70.0% 0.077 Organization 3.25 45.0% 4.00 70.0% 0.005 Relevance 3.93 62.5% 4.15 65.0% 0.347 Coverage 3.58 57.5% 4.00 67.5% 0.084 Verifiability 3.85 67.5% 3.80 67.5% 0.843 #Preferred 14 26 Table 6: Human evaluation results on 20 pairs of articles generated by STORM and oRAG. Each pair of articles is evaluated by two Wikipedia editors. The ratings are given on a scale between 1 and 7, with values \u22654 indicating good quality (see Table 10). We conduct paired t-test and report the p-value. who have made at least 500 edits on Wikipedia and have more than 1 year of experience. We randomly sample 20 topics from our dataset and evaluate the articles generated by our method and oRAG, the best baseline according to the automatic evaluation. Each pair of articles is assigned to 2 editors. We request editors to judge each article from the same five aspects defined in \u00a74.2, but using a 1 to 7 scale for more fine-grained evaluation. While our automatic evaluation uses citation quality as a proxy to evaluate Verifiability, we stick to the Wikipedia standard of \u201cverifiable with no original research\u201d in human evaluation. Besides rating the articles, editors are asked to provide open-ended feedback and pairwise preference. After the evalua- tion finishes, they are further requested to compare an article produced by our method, which they have just reviewed, with its human-written counterpart, and report their perceived usefulness of STORM using a 1-5 Likert scale. More human evaluation de- tails are included in Appendix D. Table 6 presents the rating and pairwise comparison results.11 Articles produced by STORM exhibit greater breadth and depth than oRAG outputs. In ac- cord with the finding in \u00a75.1, editors judge articles produced by STORM as more interesting, orga- nized, and having broader coverage compared to oRAG outputs. Specifically, 25% more articles pro- duced by STORM are considered organized (Orga- nization",
        "chunk_index": 8,
        "start_char": 26157,
        "end_char": 30000
      },
      {
        "id": "b5b4a195-1aed-4011-b6ad-14732ec680ca",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "to compare an article produced by our method, which they have just reviewed, with its human-written counterpart, and report their perceived usefulness of STORM using a 1-5 Likert scale. More human evaluation de- tails are included in Appendix D. Table 6 presents the rating and pairwise comparison results.11 Articles produced by STORM exhibit greater breadth and depth than oRAG outputs. In ac- cord with the finding in \u00a75.1, editors judge articles produced by STORM as more interesting, orga- nized, and having broader coverage compared to oRAG outputs. Specifically, 25% more articles pro- duced by STORM are considered organized (Orga- nization rating \u22654), and 10% more are deemed to have good coverage (Coverage rating \u22654). Even in comparison with human-written articles, one editor praises our result as providing \u201ca bit more 11For the 1-7 scale rating results on each criterion, we cal- culate the Krippendorff\u2019s Alpha to measure the inter annotator agreement (IAA), and the results are as follows: Interest Level (0.349), Organization (0.221), Relevance (0.256), Coverage (0.346), Verifiability (0.388). I think it can be specifically helpful for my pre-writing stage. I think it will help me edit a Wikipedia article for a new topic. I think it can be a potentially useful tool for the Wikipedia community. Strongly Agree Somewhat Agree Somewhat Disagree Strongly Disagree 30% 50% 20% 30% 70% 10% 60% 20% 10% Neutral Figure 3: Survey results of the perceived usefulness of STORM (n = 10). background information\u201d and another notes that \u201cI found that the AI articles had more depth compared to the Wikipedia articles\u201d. STORM also outper- forms the best baseline in pairwise comparison. More information in |R| poses challenges be- yond factual hallucination. We examine 14 pair- wise comparison responses where editors prefer oRAG outputs over STORM. Excluding 3 cases where pairwise preferences do not align with their ratings, editors assign lower Verifiability scores to articles from our approach in over 50% of the cases. Through analyzing the articles and editors\u2019 free- form feedback, we discover that low Verifiability scores stem from red herring fallacy or overspec- ulation issues. These arise when the generated articles introduce unverifiable connections between different pieces of information in |R| or between the information and the topic (examples included in Table 11). Compared to the widely discussed factual hallucination (Shuster et al., 2021; Huang et al., 2023), addressing such verifiability issues is more nuanced, surpassing basic fact-checking (Min et al., 2023). Generated articles trail behind well-revised hu- man works. While STORM outperforms the oRAG baseline, editors comment that the generated articles are less informative than actual Wikipedia pages. Another major issue identified is the trans- fer of bias and tone from Internet sources to the generated article, with 7 out of 10 editors men- tioning that the STORM-generated articles sound \u201cemotional\u201d or \u201cunneutral\u201d. More analysis is dis- cussed in Appendix E. This feedback suggests that reducing the retrieval bias in the pre-writing stage is a worthwhile direction for future work. Generated articles are a good starting point. As shown in Figure 3, editors are unanimous in agree- ing that STORM can aid them in their pre-writing stage. It is gratifying to know that the tool is help- ful to experienced editors. 80% of the editors think that STORM can help them edit a Wikipedia article for a new topic. More reservation is expressed to the usefulness of STORM for the Wikipedia com- munity at large; nonetheless, 70% of the editors think it is useful, with only 10% disagreeing. 7 Related Works Retrieval-Augmented Generation (RAG) Aug- menting language models (LMs) with retrieval at inference time is a typical way to leverage",
        "chunk_index": 9,
        "start_char": 29352,
        "end_char": 33187
      },
      {
        "id": "4aea013c-f23c-4140-8c1a-53485f08d6f7",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "Figure 3, editors are unanimous in agree- ing that STORM can aid them in their pre-writing stage. It is gratifying to know that the tool is help- ful to experienced editors. 80% of the editors think that STORM can help them edit a Wikipedia article for a new topic. More reservation is expressed to the usefulness of STORM for the Wikipedia com- munity at large; nonetheless, 70% of the editors think it is useful, with only 10% disagreeing. 7 Related Works Retrieval-Augmented Generation (RAG) Aug- menting language models (LMs) with retrieval at inference time is a typical way to leverage exter- nal knowledge stores (Ram et al., 2023; Izacard et al., 2023). While some works use retrieval to construct demonstrations for in-context learn- ing (Li et al., 2023; Liu et al., 2022; Agrawal et al., 2023; Poesia et al., 2022; Shi et al., 2022; Khattab et al., 2022), another line of works uses retrieval to provide additional information for LMs to ground on. Lewis et al. (2020) study RAG on knowledge- intensive NLP tasks and find it improves diver- sity and factuality. Semnani et al. (2023) de- signs a RAG-based chatbot grounded on English Wikipedia to stop LLM-based chatbots from hal- lucination. Besides, RAG can be used to generate text with citations (Menick et al., 2022; Gao et al., 2023) and build attributed question answering sys- tems (Bohnet et al., 2023). While RAG is widely studied in question answering, how to use it for long-form article generation is less investigated. As a general framework, RAG is flexible in both the retrieval source and time. The retrieval sources can vary from domain databases (Zakka et al., 2023), code documentation (Zhou et al., 2023), to the whole Internet (Nakano et al., 2022; Komeili et al., 2022). Regarding the time, besides a one- time retrieval before generation, the system can be designed to self-decide when to retrieve across the course of the generation (Jiang et al., 2023b; Parisi et al., 2022; Shuster et al., 2022; Yao et al., 2023). Automatic Expository Writing Different from other types of long-form generation (Yang et al., 2022; Feng et al., 2018), automatic expository writ- ing requires grounding on external documents and leveraging the interplay between reading and writ- ing. Balepur et al. (2023) propose the Imitate- Retrieve-Paraphrase framework for expository writ- ing at the paragraph level to address the challenges in synthesizing information from multiple sources. Beyond summarizing sources, Shen et al. (2023) highlight that expository writing requires the au- thor\u2019s sensemaking process over source documents and good outline planning. We tackle these chal- lenges by focusing on the pre-writing stage. Question Asking in NLP Question asking capa- bilities in NLP systems have expanded across sev- eral fronts, including generating clarification ques- tions to understand user intents (Aliannejadi et al., 2019; Rahmani et al., 2023), and breaking large questions into smaller ones to improve composi- tional reasoning (Press et al., 2023). While humans usually ask questions to learn new knowledge (Taw- fik et al., 2020; Booth et al., 2003), how to opti- mize question informativeness and specificity in information-seeking conversations remains less ex- plored. The closest work is Qi et al. (2020) which defines the question informativeness using the un- igram precision function and uses reinforcement learning to increase the question informativeness. 8 Conclusion We propose STORM, an LLM-based writing sys- tem that automates the pre-writing stage for creat- ing Wikipedia-like articles from scratch. We cu- rate the FreshWiki dataset and establish evaluation criteria to study the generation of grounded long- form articles. Experimental results demonstrate that the question asking mechanism in STORM improves both the outline",
        "chunk_index": 10,
        "start_char": 32596,
        "end_char": 36423
      },
      {
        "id": "f549e3e9-604a-42e0-b68e-f5e6c4711bad",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "al., 2003), how to opti- mize question informativeness and specificity in information-seeking conversations remains less ex- plored. The closest work is Qi et al. (2020) which defines the question informativeness using the un- igram precision function and uses reinforcement learning to increase the question informativeness. 8 Conclusion We propose STORM, an LLM-based writing sys- tem that automates the pre-writing stage for creat- ing Wikipedia-like articles from scratch. We cu- rate the FreshWiki dataset and establish evaluation criteria to study the generation of grounded long- form articles. Experimental results demonstrate that the question asking mechanism in STORM improves both the outline and article quality. With the improved breadth and depth, STORM helps surface new challenges for grounded writing sys- tems through expert evaluation. The experienced Wikipedia editors in our study unanimously agree that STORM is helpful for their pre-writing stage. Limitations In this work, we explore generating Wikipedia- like articles from scratch as a way to push the frontier of automatic expository writing and long- form article generation. While our approach sig- nificantly outperforms baseline methods in both automatic and human evaluations, the quality of machine-written articles still lags behind well- revised human-authored articles, specifically in aspects of neutrality and verifiability. Although STORM discovers different perspectives in re- searching the given topic, the collected information may still be biased towards dominant sources on the Internet and may contain promotional content. Moreover, the verifiability issues identified in this work go beyond factual hallucination, which high- lights new challenges to grounded writing systems. Another limitation of this work is that although we focus on the task of generating Wikipedia-like articles from scratch, our task setup is still simpli- fied to only consider the generation of free-form text. Human-authored high-quality Wikipedia ar- ticles usually contain structured data and multi- modal information. We leave the exploration of generating multi-modal grounded articles for fu- ture work. Acknowledgements We thank You.com for generously providing the search API that supported our experiments. We also thank Sina J. Semnani, Shicheng Liu, Eric Ze- likman for providing helpful feedback and the ACL ARR reviewers for their valuable comments. This work is supported in part by the Verdant Founda- tion and Microsoft Azure AI credits. Yijia Shao is supported by a Stanford School of Engineering Fellowship. Ethics Statement Different from the creative generation, grounded ar- ticle generation may impact how people learn about topics or consume source information. All the stud- ies and the evaluation in this work are designed to prevent the dissemination of misinformation by not publishing generated content online and im- plementing strict accuracy checks. We avoid any disruption to Wikipedia or related communities, as our system does not interact with live pages. Also, although we try to generate grounded articles, we believe there is no privacy issue related to this work as we only use information publicly available on the Internet. The primary risk of our work is that the Wikipedia articles written by our system are grounded on information on the Internet which contains some biased or discriminative content on its own. Currently, our system relies on the search engine to retrieve information but does not include any post-processing module. We believe improv- ing the retrieval module to have good coverage of different viewpoints and adding a content sifting module to the current system will be a critical next step to achieve better neutrality and balance in the generated articles. Another limitation we see from an ethical point of view is that we only consider writing English Wikipedia articles in this work. Extending the cur- rent system to a multilingual setup is a meaningful direction for",
        "chunk_index": 11,
        "start_char": 35719,
        "end_char": 39731
      },
      {
        "id": "6a4c8aba-d149-47ed-ad9b-abae5c5e1727",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "or discriminative content on its own. Currently, our system relies on the search engine to retrieve information but does not include any post-processing module. We believe improv- ing the retrieval module to have good coverage of different viewpoints and adding a content sifting module to the current system will be a critical next step to achieve better neutrality and balance in the generated articles. Another limitation we see from an ethical point of view is that we only consider writing English Wikipedia articles in this work. Extending the cur- rent system to a multilingual setup is a meaningful direction for future work as more topics do not have Wikipedia pages in non-English languages. References Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2023. In- context examples selection for machine translation. In Findings of the Association for Computational Linguistics: ACL 2023, pages 8857\u20138873, Toronto, Canada. Association for Computational Linguistics. Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland Vollgraf. 2019. FLAIR: An easy-to-use framework for state-of-the- art NLP. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 54\u201359, Minneapolis, Minnesota. Association for Computational Linguistics. Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W Bruce Croft. 2019. Asking clari- fying questions in open-domain information-seeking conversations. In Proceedings of the 42nd interna- tional acm sigir conference on research and develop- ment in information retrieval, pages 475\u2013484. Nishant Balepur, Jie Huang, and Kevin Chang. 2023. Expository text generation: Imitate, retrieve, para- phrase. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Process- ing, pages 11896\u201311919, Singapore. Association for Computational Linguistics. Siddhartha Banerjee and Prasenjit Mitra. 2015. WikiKreator: Improving Wikipedia stubs automat- ically. In Proceedings of the 53rd Annual Meet- ing of the Association for Computational Linguis- tics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Pa- pers), pages 867\u2013877, Beijing, China. Association for Computational Linguistics. Bernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aha- roni, Daniel Andor, Livio Baldini Soares, Massimil- iano Ciaramita, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Lierni Sestorain Saralegui, Tal Schus- ter, William W. Cohen, Michael Collins, Dipanjan Das, Donald Metzler, Slav Petrov, and Kellie Webster. 2023. Attributed question answering: Evaluation and modeling for attributed large language models. Wayne C Booth, Gregory G Colomb, and Joseph M Williams. 2003. The craft of research. University of Chicago press. Laura Dietz and John Foley. 2019. Trec car y3: Com- plex answer retrieval overview. In Proceedings of Text REtrieval Conference (TREC). Christina S Doyle. 1994. Information literacy in an information society: A concept for the information age. Diane Publishing. Ann-Marie Eriksson and \u00c5sa M\u00e4kitalo. 2015. Supervi- sion at the outline stage: Introducing and encounter- ing issues of sustainable development through aca- demic writing assignments. Text & Talk, 35(2):123\u2013 153. Angela Fan and Claire Gardent. 2022. Generating bi- ographies on Wikipedia: The impact of gender bias on the retrieval-based generation of women biogra- phies. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 8561\u20138576, Dublin, Ireland. Association for Computational Linguistics. Xiaocheng Feng, Ming Liu, Jiahao Liu, Bing Qin, Yibo Sun, and Ting Liu. 2018. Topic-to-essay generation with neural networks. In IJCAI, pages 4078\u20134084. Tira Nur Fitria. 2023. Artificial intelligence (ai) tech- nology in openai chatgpt application: A review of chatgpt in writing english essay. In ELT Forum: Jour- nal of English Language Teaching, volume 12, pages 44\u201358. Pasi Fr\u00e4nti and Radu Mariescu-Istodor. 2023. Soft preci- sion and recall. Pattern Recognition Letters, 167:115\u2013 121. R",
        "chunk_index": 12,
        "start_char": 39111,
        "end_char": 43327
      },
      {
        "id": "1fdfd9b0-40f8-4b12-87b6-e7189a98b111",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 8561\u20138576, Dublin, Ireland. Association for Computational Linguistics. Xiaocheng Feng, Ming Liu, Jiahao Liu, Bing Qin, Yibo Sun, and Ting Liu. 2018. Topic-to-essay generation with neural networks. In IJCAI, pages 4078\u20134084. Tira Nur Fitria. 2023. Artificial intelligence (ai) tech- nology in openai chatgpt application: A review of chatgpt in writing english essay. In ELT Forum: Jour- nal of English Language Teaching, volume 12, pages 44\u201358. Pasi Fr\u00e4nti and Radu Mariescu-Istodor. 2023. Soft preci- sion and recall. Pattern Recognition Letters, 167:115\u2013 121. R Edward Freeman, Jeffrey S Harrison, Andrew C Wicks, Bidhan L Parmar, and Simone De Colle. 2010. Stakeholder theory: The state of the art. Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations. In Proceedings of the 2023 Con- ference on Empirical Methods in Natural Language Processing, pages 6465\u20136488, Singapore. Associa- tion for Computational Linguistics. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023. A survey on hallucination in large lan- guage models: Principles, taxonomy, challenges, and open questions. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi- Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research, 24(251):1\u201343. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men- sch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guil- laume Lample, Lucile Saulnier, et al. 2023a. Mistral 7b. arXiv preprint arXiv:2310.06825. Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023b. Active retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Lan- guage Processing, pages 7969\u20137992, Singapore. As- sociation for Computational Linguistics. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023. Large language models struggle to learn long-tail knowledge. In In- ternational Conference on Machine Learning, pages 15696\u201315707. PMLR. Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-search- predict: Composing retrieval and language mod- els for knowledge-intensive NLP. arXiv preprint arXiv:2212.14024. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vard- hamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Za- haria, and Christopher Potts. 2023. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. 2023. Prometheus: Inducing fine-grained evalua- tion capability in language models. arXiv preprint arXiv:2310.08491. Mojtaba Komeili, Kurt Shuster, and Jason Weston. 2022. Internet-augmented dialogue generation. In Proceed- ings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 8460\u20138478, Dublin, Ireland. Association for Computational Linguistics. Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo. 2023. LongEval: Guidelines for human evaluation of faithfulness in long-form summarization. In Proceed- ings of the 17th Conference of the European Chap- ter of the Association for Computational Linguistics, pages 1650\u20131669, Dubrovnik, Croatia. Association for Computational Linguistics. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock- t\u00e4schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neu- ral Information Processing Systems, 33:9459\u20139474. Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni,",
        "chunk_index": 13,
        "start_char": 42641,
        "end_char": 46987
      },
      {
        "id": "69b169a7-fa17-499b-81a9-67b791bfd778",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo. 2023. LongEval: Guidelines for human evaluation of faithfulness in long-form summarization. In Proceed- ings of the 17th Conference of the European Chap- ter of the Association for Computational Linguistics, pages 1650\u20131669, Dubrovnik, Croatia. Association for Computational Linguistics. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock- t\u00e4schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neu- ral Information Processing Systems, 33:9459\u20139474. Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023. Unified demonstration retriever for in- context learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 4644\u20134668, Toronto, Canada. Association for Computational Lin- guistics. Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extrac- tion and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online. Associa- tion for Computational Linguistics. Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating wikipedia by summariz- ing long sequences. In International Conference on Learning Representations. Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell- Gillingham, Geoffrey Irving, and Nat McAleese. 2022. Teaching language models to support answers with verified quotes. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle- moyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12076\u201312100, Singa- pore. Association for Computational Linguistics. Juli\u00e0 Minguill\u00f3n, Maura Lerga, Eduard Aibar, Josep Llad\u00f3s-Masllorens, and Antoni Meseguer-Artola. 2017. Semi-automatic generation of a corpus of wikipedia articles on science and technology. Profe- sional de la Informaci\u00f3n, 26(5):995\u20131005. Rosa Munoz-Luna. 2015. Main ingredients for suc- cess in l2 academic writing: Outlining, drafting and proofreading. PloS one, 10(6):e0128309. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2022. Webgpt: Browser- assisted question-answering with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instruc- tions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744. Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm: Tool augmented language models. John V Pavlik. 2023. Collaborating with chatgpt: Con- sidering the implications of generative artificial intel- ligence for journalism and media education. Journal- ism & Mass Communication Educator, 78(1):84\u201393. Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gul- wani. 2022. Synchromesh: Reliable code generation from pre-trained language models. In International Conference on Learning Representations. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language mod- els. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5687\u20135711, Singa- pore. Association for Computational Linguistics. Peng Qi, Yuhao Zhang, and Christopher D. Manning. 2020. Stay hungry, stay",
        "chunk_index": 14,
        "start_char": 46255,
        "end_char": 50670
      },
      {
        "id": "19cf03b1-8151-411e-bca1-730301eeabbe",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "artificial intel- ligence for journalism and media education. Journal- ism & Mass Communication Educator, 78(1):84\u201393. Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gul- wani. 2022. Synchromesh: Reliable code generation from pre-trained language models. In International Conference on Learning Representations. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language mod- els. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5687\u20135711, Singa- pore. Association for Computational Linguistics. Peng Qi, Yuhao Zhang, and Christopher D. Manning. 2020. Stay hungry, stay focused: Generating infor- mative and specific questions in information-seeking conversations. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 25\u2013 40, Online. Association for Computational Linguis- tics. Hongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu, Xinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Webbrain: Learning to generate factually correct articles for queries by grounding on large web corpus. Hossein A. Rahmani, Xi Wang, Yue Feng, Qiang Zhang, Emine Yilmaz, and Aldo Lipani. 2023. A survey on asking clarification questions datasets in conversa- tional systems. In Proceedings of the 61st Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 2698\u20132716, Toronto, Canada. Association for Computational Lin- guistics. Ashwin Ram. 1991. A theory of questions and question asking. Journal of the Learning Sciences, 1(3-4):273\u2013 318. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented lan- guage models. Transactions of the Association for Computational Linguistics. Nils Reimers and Iryna Gurevych. 2019. Sentence- BERT: Sentence embeddings using Siamese BERT- networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China. Association for Com- putational Linguistics. D Gordon Rohman. 1965. Pre-writing the stage of dis- covery in the writing process. College composition and communication, 16(2):106\u2013112. Christina Sauper and Regina Barzilay. 2009. Auto- matically generating Wikipedia articles: A structure- aware approach. In Proceedings of the Joint Con- ference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 208\u2013216, Suntec, Singapore. Association for Computational Linguistics. Sina Semnani, Violet Yao, Heidi Zhang, and Monica Lam. 2023. WikiChat: Stopping the hallucination of large language model chatbots by few-shot ground- ing on Wikipedia. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2387\u20132413, Singapore. Association for Computa- tional Linguistics. Zejiang Shen, Tal August, Pao Siangliulue, Kyle Lo, Jonathan Bragg, Jeff Hammerbacher, Doug Downey, Joseph Chee Chang, and David Sontag. 2023. Be- yond summarization: Designing ai support for real- world expository writing tasks. Weijia Shi, Julian Michael, Suchin Gururangan, and Luke Zettlemoyer. 2022. Nearest neighbor zero-shot inference. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3254\u20133265, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston. 2022. Language models that seek for knowledge: Modular search & generation for dialogue and prompt completion. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 373\u2013393, Abu Dhabi, United Arab Emirates. Associ- ation for Computational Linguistics. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784\u20133803, Punta Cana, Do- minican Republic. Association for Computational Linguistics. Christine M Tardy. 2010. Writing for the world: Wikipedia as an introduction to",
        "chunk_index": 15,
        "start_char": 49933,
        "end_char": 54317
      },
      {
        "id": "f70fee07-3150-4d8d-b09d-66a3709efb58",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston. 2022. Language models that seek for knowledge: Modular search & generation for dialogue and prompt completion. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 373\u2013393, Abu Dhabi, United Arab Emirates. Associ- ation for Computational Linguistics. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784\u20133803, Punta Cana, Do- minican Republic. Association for Computational Linguistics. Christine M Tardy. 2010. Writing for the world: Wikipedia as an introduction to academic writing. In English teaching forum, volume 48, page 12. ERIC. Andrew A Tawfik, Arthur Graesser, Jessica Gatewood, and Jaclyn Gishbaugher. 2020. Role of questions in inquiry-based instruction: towards a design taxon- omy for question-asking and implications for design. Educational Technology Research and Development, 68:653\u2013678. Charles A Weaver III and Walter Kintsch. 1991. Expos- itory text. Karsten Wenzlaff and Sebastian Spaeth. 2022. Smarter than humans? validating how openai\u2019s chatgpt model explains crowdfunding, alternative finance and com- munity finance. Validating how OpenAI\u2019s ChatGPT model explains Crowdfunding, Alternative Finance and Community Finance.(December 22, 2022). Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023. A critical evaluation of evaluations for long-form question answering. In Proceedings of the 61st Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 3225\u20133245, Toronto, Canada. Association for Com- putational Linguistics. Kevin Yang, Dan Klein, Nanyun Peng, and Yuandong Tian. 2023. DOC: Improving long story coherence with detailed outline control. In Proceedings of the 61st Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 3378\u20133465, Toronto, Canada. Association for Com- putational Linguistics. Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. 2022. Re3: Generating longer stories with recursive reprompting and revision. In Proceedings of the 2022 Conference on Empirical Methods in Nat- ural Language Processing, pages 4393\u20134479, Abu Dhabi, United Arab Emirates. Association for Com- putational Linguistics. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations. Cyril Zakka, Akash Chaurasia, Rohan Shad, Alex R Dalal, Jennifer L Kim, Michael Moor, Kevin Alexan- der, Euan Ashley, Jack Boyd, Kathleen Boyd, et al. 2023. Almanac: Retrieval-augmented language mod- els for clinical medicine. Research Square. Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig. 2023. Docprompting: Gener- ating code by retrieving the docs. In The Eleventh International Conference on Learning Representa- tions. Average Numer of Sections 8.4 Average Number of All-level Headings 15.8 Average Length of a Section 327.8 Average Length of Total Article 2159.1 Average Number of References 90.1 Table 7: Statistics of the dataset used in our experiments. Figure 4: Evolution of reference count in the Wikipedia article editing process. A Dataset Details As discussed in \u00a72.1, we curate the FreshWiki dataset by collecting recent and high-quality En- glish Wikipedia articles. We select the most-edited pages over a specific period rather than using cre- ation dates as a cutoff because most of Wikipedia articles are \u201cstubs\u201d or are of low quality when they were created. For quality, we consider articles pre- dicted to be of B-class quality or above. According to Wikipedia statistics12, only around 3% of ex- isting Wikipedia pages meet this quality standard. As LLMs can generate reasonably good outputs, we think it is important to use high-quality human- written articles as references for further research. For experiments in this work, we randomly se- lect 100 samples with human-written articles",
        "chunk_index": 16,
        "start_char": 53580,
        "end_char": 57761
      },
      {
        "id": "d8b83bca-9d81-4fb1-963f-9806b8051bb6",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "select the most-edited pages over a specific period rather than using cre- ation dates as a cutoff because most of Wikipedia articles are \u201cstubs\u201d or are of low quality when they were created. For quality, we consider articles pre- dicted to be of B-class quality or above. According to Wikipedia statistics12, only around 3% of ex- isting Wikipedia pages meet this quality standard. As LLMs can generate reasonably good outputs, we think it is important to use high-quality human- written articles as references for further research. For experiments in this work, we randomly se- lect 100 samples with human-written articles un- der 3000 words to have a meaningful comparison. Table 7 gives the data statistics. Notably, human- authored articles have a large number of references but they require numerous edits to achieve this. Fig- ure 4 illustrates the evolution of the reference count in the article edit process and Figure 5 gives the dis- tribution of edit counts for human-authored articles used in our experiments. 12https://en.wikipedia.org/wiki/Wikipedia: Content_assessment Figure 5: Distribution of edit counts for Wikipedia arti- cles in our experiments (n = 100). B Pseudo Code of STORM In \u00a73, we introduce STORM, a framework that au- tomates the pre-writing stage by discovering differ- ent perspectives, simulating information-seeking conversations, and creating a comprehensive out- line. Algorithm 1 displays the skeleton of STORM. We implement STORM with zero-shot prompt- ing using the DSPy framework (Khattab et al., 2023). Listing 1 and 2 show the prompts used in our implementation. We highlight that STORM offers a general framework designed to assist the creation of grounded, long-form articles, without depending extensively on prompt engineering for a single domain. C Automatic Evaluation Details C.1 Soft Heading Recall We calculate the soft heading recall between the multi-level headings in the generated outline, con- sidered as the prediction P, and those in the human- written article, considered as the ground truth G. The calculation is based on the soft recall defini- tion in Fr\u00e4nti and Mariescu-Istodor (2023). Given a set A = {Ai}K i=1, soft count of an item is defined as the inverse of the sum of its similarity to other items in the set: count (Ai) = 1 PK j=1 Sim (Ai, Aj) Sim (Ai, Aj) = cos (embed(Ai), embed(Aj)) , (1) where embed(\u00b7) in Equation (1) is parameterized by paraphrase-MiniLM-L6-v2 provided in the Sentence-Transformers library13. The cardinality 13https://huggingface.co/sentence-transformers/ paraphrase-MiniLM-L6-v2 1 class GenRelatedTopicsPrompt(dspy.Signature): 2 \"\"\" 3 I'm writing a Wikipedia page for a topic mentioned below. Please identify and recommend some Wikipedia pages on closely related subjects. I'm looking for examples that provide insights into interesting aspects commonly associated with this topic , or examples that help me understand the typical content and structure included in Wikipedia pages for similar topics. 4 Please list the urls in separate lines. 5 \"\"\" 6 7 topic = dspy.InputField(prefix =\" Topic of interest:\", format=str) 8 related_topics = dspy.OutputField () 9 10 class GenPerspectivesPrompt(dspy.Signature): 11 \"\"\" 12 You need to select a group of Wikipedia editors who will work together to create a comprehensive article on the topic. Each of them represents a different perspective , role , or affiliation related to this topic. You can use other Wikipedia pages of related topics for inspiration. For each editor , add description of what they will focus on. 13 Give your answer in the following format: 1. short summary of editor 1: description\\n2. short summary of editor 2: description\\n... 14 \"\"\" 15 16 topic = dspy.InputField(prefix='Topic of interest:', format=str) 17 examples = dspy.InputField(prefix='Wiki page outlines of related topics for inspiration :\\n', format=str) 18 perspectives = dspy.OutputField () 19 20",
        "chunk_index": 17,
        "start_char": 57137,
        "end_char": 61064
      },
      {
        "id": "3c1281c2-7263-459e-8800-6c4c1d110d6d",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "work together to create a comprehensive article on the topic. Each of them represents a different perspective , role , or affiliation related to this topic. You can use other Wikipedia pages of related topics for inspiration. For each editor , add description of what they will focus on. 13 Give your answer in the following format: 1. short summary of editor 1: description\\n2. short summary of editor 2: description\\n... 14 \"\"\" 15 16 topic = dspy.InputField(prefix='Topic of interest:', format=str) 17 examples = dspy.InputField(prefix='Wiki page outlines of related topics for inspiration :\\n', format=str) 18 perspectives = dspy.OutputField () 19 20 class GenQnPrompt(dspy.Signature): 21 \"\"\" 22 You are an experienced Wikipedia writer and want to edit a specific page. Besides your identity as a Wikipedia writer , you have a specific focus when researching the topic. 23 Now , you are chatting with an expert to get information. Ask good questions to get more useful information. 24 When you have no more question to ask , say \"Thank you so much for your help!\" to end the conversation. 25 Please only ask one question at a time and don't ask what you have asked before. Your questions should be related to the topic you want to write. 26 \"\"\" 27 28 topic = dspy.InputField(prefix='Topic you want to write: ', format=str) 29 persona = dspy.InputField(prefix='Your specific perspective: ', format=str) 30 conv = dspy.InputField(prefix='Conversation history :\\n', format=str) 31 question = dspy.OutputField () 32 33 class GenQueriesPrompt(dspy.Signature): 34 \"\"\" 35 You want to answer the question using Google search. What do you type in the search box? 36 Write the queries you will use in the following format:- query 1\\n- query 2\\n... 37 \"\"\" 38 39 topic = dspy.InputField(prefix='Topic you are discussing about: ', format=str) 40 question = dspy.InputField(prefix='Question you want to answer: ', format=str) 41 queries = dspy.OutputField () Listing 1: Prompts used in STORM, corresponding to Line 4, 11, 19, 22 in Algorithm 1. 1 class GenAnswerPrompt(dspy.Signature): 2 \"\"\" 3 You are an expert who can use information effectively. You are chatting with a Wikipedia writer who wants to write a Wikipedia page on topic you know. You have gathered the related information and will now use the information to form a response. 4 Make your response as informative as possible and make sure every sentence is supported by the gathered information. 5 \"\"\" 6 7 topic = dspy.InputField(prefix='Topic you are discussing about:', format=str) 8 conv = dspy.InputField(prefix='Question :\\n', format=str) 9 info = dspy.InputField( 10 prefix='Gathered information :\\n', format=str) 11 answer = dspy.OutputField(prefix='Now give your response :\\n') 12 13 14 class DirectGenOutlinePrompt(dspy.Signature): 15 \"\"\" 16 Write an outline for a Wikipedia page. 17 Here is the format of your writing: 18 1. Use \"#\" Title\" to indicate section title , \"##\" Title\" to indicate subsection title , \"###\" Title\" to indicate subsubsection title , and so on. 19 2. Do not include other information. 20 \"\"\" 21 22 topic = dspy.InputField(prefix =\" Topic you want to write: \", format=str) 23 outline = dspy.OutputField(prefix =\" Write the Wikipedia page outline :\\n\") 24 25 26 class RefineOutlinePrompt(dspy.Signature): 27 \"\"\" 28 Improve an outline for a Wikipedia page. You already have a draft outline that covers the general information. Now you want to improve it based on the information learned from an information -seeking conversation to make it more comprehensive. 29 Here is the format of your writing: 30 1. Use \"#\" Title\" to indicate section title , \"##\" Title\" to indicate subsection title , \"###\" Title\" to indicate subsubsection title , and so on. 31 2. Do not include other information. 32 \"\"\" 33",
        "chunk_index": 18,
        "start_char": 60411,
        "end_char": 64192
      },
      {
        "id": "186315f4-6561-4d74-884e-57d58f3e774f",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "dspy.OutputField(prefix =\" Write the Wikipedia page outline :\\n\") 24 25 26 class RefineOutlinePrompt(dspy.Signature): 27 \"\"\" 28 Improve an outline for a Wikipedia page. You already have a draft outline that covers the general information. Now you want to improve it based on the information learned from an information -seeking conversation to make it more comprehensive. 29 Here is the format of your writing: 30 1. Use \"#\" Title\" to indicate section title , \"##\" Title\" to indicate subsection title , \"###\" Title\" to indicate subsubsection title , and so on. 31 2. Do not include other information. 32 \"\"\" 33 34 topic = dspy.InputField(prefix =\" Topic you want to write: \", format=str) 35 conv = dspy.InputField(prefix =\" Conversation history :\\n\", format=str) 36 old_outline = dspy.OutputField(prefix =\" Current outline :\\n\", format=str) 37 outline = dspy.OutputField( 38 prefix='Write the Wikipedia page outline :\\n') Listing 2: Prompts used in STORM (continue), corresponding to Line 24, 31, 32 in Algorithm 1. Algorithm 1: STORM Input :Topic t, maximum perspective N, maximum conversation round M Output :Outline O, references R 1 P0 = \"basic fact writer ...\" // Constant. 2 R \u2190[ ] 3 // Discover perspectives P. 4 related_topics \u2190gen_related_topics(t) 5 tocs \u2190[ ] 6 foreach related_t in related_topics do 7 article \u2190get_wiki_article(related_t) if article then 8 tocs.append(extract_toc(article)) 9 end 10 end 11 P \u2190gen_perspectives(t, tocs) 12 P \u2190[P0] + P[:N] 13 // Simulate conversations. 14 convos \u2190[ ] 15 foreach p in P do 16 convo_history \u2190[ ] 17 for i = 1 to M do 18 // Question asking. 19 q \u2190gen_qn(t, p, dlg_history) 20 convo_history.append(q) 21 // Question answering. 22 queries \u2190gen_queries(t, q) 23 sources \u2190 search_and_sift(queries) 24 a \u2190gen_ans(t, q, sources) 25 convo_history.append(a) 26 R.append(sources) 27 end 28 convos.append(convo_history) 29 end 30 // Create the outline. 31 OD \u2190direct_gen_outline(t) 32 O \u2190refine_outline(t, OD, convos) 33 return O, R of A is the sum of the counts of its individual items: card(A) = K X i=1 count (Ai) (2) The soft heading recall is calculated as soft heading recall = card(G \u2229P) card(G) , (3) where the cardinality of intersection is defined via the union as follows: card(G \u2229P) = card(G) + card(P) \u2212card(G \u222aP). (4) C.2 LLM Evaluator We use Prometheus14 (Kim et al., 2023), a 13B open-source evaluator LLM that can assess long- form text based on customized 1-5 scale rubric, to grade the article from the aspects of Interest level, Coherence and Organization, Relevance and Fo- cus, and Coverage. Table 8 gives our grading rubric. While Prometheus is best used with a score 5 ref- erence answer, we find adding the reference will exceed the context length limit of the model. Since Kim et al. (2023) show Prometheus ratings without reference also correlate well with human prefer- ences, we omit the reference and trim the input article to be within 2000 words by iteratively re- moving contents from the shortest section to ensure the input can fit into the model\u2019s context window. C.3 More Discussion of the Citation Quality Incorrectly Split 12% False Negative 15% Lack Citation 47% Improper Inferential Linking Irrelevant Source Others 1% Inaccurate Paraphrasing 14% 7% 4% Figure 6: Error analysis of unsupported sentences in 10 sampled articles. 14https://huggingface.co/kaist-ai/ prometheus-13b-v1.0 Criteria Description Interest Level: How engaging and thought-provoking is the article? Score 1 Description Not engaging at all; no attempt to capture the reader\u2019s attention. Score 2 Description Fairly engaging with a basic narrative but lacking depth. Score 3 Description Moderately engaging with several interesting points. Score 4 Description Quite engaging with a well-structured narrative and noteworthy points that frequently capture and retain attention. Score",
        "chunk_index": 19,
        "start_char": 63582,
        "end_char": 67419
      },
      {
        "id": "72533a72-bf6f-4a05-a56d-5479a7acb355",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "Quality Incorrectly Split 12% False Negative 15% Lack Citation 47% Improper Inferential Linking Irrelevant Source Others 1% Inaccurate Paraphrasing 14% 7% 4% Figure 6: Error analysis of unsupported sentences in 10 sampled articles. 14https://huggingface.co/kaist-ai/ prometheus-13b-v1.0 Criteria Description Interest Level: How engaging and thought-provoking is the article? Score 1 Description Not engaging at all; no attempt to capture the reader\u2019s attention. Score 2 Description Fairly engaging with a basic narrative but lacking depth. Score 3 Description Moderately engaging with several interesting points. Score 4 Description Quite engaging with a well-structured narrative and noteworthy points that frequently capture and retain attention. Score 5 Description Exceptionally engaging throughout, with a compelling narrative that consistently stimulates interest. Criteria Description Coherence and Organization: Is the article well-organized and logically structured? Score 1 Description Disorganized; lacks logical structure and coherence. Score 2 Description Fairly organized; a basic structure is present but not consistently followed. Score 3 Description Organized; a clear structure is mostly followed with some lapses in coherence. Score 4 Description Good organization; a clear structure with minor lapses in coherence. Score 5 Description Excellently organized; the article is logically structured with seamless transitions and a clear argument. Criteria Description Relevance and Focus: Does the article stay on topic and maintain a clear focus? Score 1 Description Off-topic; the content does not align with the headline or core subject. Score 2 Description Somewhat on topic but with several digressions; the core subject is evident but not consistently adhered to. Score 3 Description Generally on topic, despite a few unrelated details. Score 4 Description Mostly on topic and focused; the narrative has a consistent relevance to the core subject with infrequent digressions. Score 5 Description Exceptionally focused and entirely on topic; the article is tightly centered on the subject, with every piece of information contributing to a comprehensive understanding of the topic. Criteria Description Broad Coverage: Does the article provide an in-depth exploration of the topic and have good coverage? Score 1 Description Severely lacking; offers little to no coverage of the topic\u2019s primary aspects, resulting in a very narrow perspective. Score 2 Description Partial coverage; includes some of the topic\u2019s main aspects but misses others, resulting in an incomplete portrayal. Score 3 Description Acceptable breadth; covers most main aspects, though it may stray into minor unnecessary details or overlook some relevant points. Score 4 Description Good coverage; achieves broad coverage of the topic, hitting on all major points with minimal extraneous information. Score 5 Description Exemplary in breadth; delivers outstanding coverage, thoroughly detailing all crucial aspects of the topic without including irrelevant information. Table 8: Scoring rubrics on a 1-5 scale for the evaluator LLM. Error Type Topic Unsupported Sentence Source Improper Inferential Linking Lahaina, Hawaii Throughout its history, religion has remained the paramount aspect of Hawaiian life in Lahaina , permeating every daily activity and significant event[5]. [5] \u201cReligion, Beliefs & Spirituality\u201d (The source discusses religion as part of Hawaiian life but does not mention Lahania .) Inaccurate Paraphrasing 2022 Crimean Bridge explosion Completed in June 2020 , the bridge serves as a major supply route for Russian forces in the region and is significant to Russia\u2019s claim over the disputed territory[2][11]. [2] \u201cCrimean Bridge - Wikipedia\u201d (The source says \u201cThe first scheduled passenger train crossed the bridge on 25 December 2019, while the bridge was opened for freight trains on 30 June 2020 \u201d.) Citing Irrelevant Sources LK-99 For example, comparisons have been drawn between the performance of LK-9 and the dynamic resolution capabilities of video games such as Battlefield 2042[22]. [22] \u201cBattlefield 2042 PC performance guide: The best settings for a high",
        "chunk_index": 20,
        "start_char": 66665,
        "end_char": 70843
      },
      {
        "id": "74a5612f-98a2-4d5b-91d7-ac2d89a4a666",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "in June 2020 , the bridge serves as a major supply route for Russian forces in the region and is significant to Russia\u2019s claim over the disputed territory[2][11]. [2] \u201cCrimean Bridge - Wikipedia\u201d (The source says \u201cThe first scheduled passenger train crossed the bridge on 25 December 2019, while the bridge was opened for freight trains on 30 June 2020 \u201d.) Citing Irrelevant Sources LK-99 For example, comparisons have been drawn between the performance of LK-9 and the dynamic resolution capabilities of video games such as Battlefield 2042[22]. [22] \u201cBattlefield 2042 PC performance guide: The best settings for a high frame rate\u201d ( The source is irrelevant to LK-99. ) Table 9: Examples of different error types of unsupported sentences. We use Mistral 7B-Instruct15 (Jiang et al., 2023a) to examine whether the cited passages entail the generated sentence. Table 4 reports the citation quality of articles produced by our approach, show- ing that around 15% sentences in generated articles are unsupported by citations. We further investi- gate the failure cases by randomly sampling 10 articles and an author manually examines all the unsupported sentences in these articles. Besides sentences that are incorrectly split16, lack citations, or are deemed supported by the author\u2019s judgment, our analysis identifies three main error categories (examples are given in Table 9): improper inferen- tial linking, inaccurate paraphrasing, and citing irrelevant sources. We show the error distribution in Figure 6. No- tably, the most common errors stem from the ten- dency of LLMs to form improper inferential links between different pieces of information presented in the context window. Our analysis of citation quality suggests that, in addition to avoiding hallu- cinations, future research in grounded text gener- ation should also focus on preventing LLMs from making overly inferential leaps based on the pro- vided information. D Human Evaluation Details We recruited 10 experienced Wikipedia editors to participate in our study by creating a research page on Meta-Wiki17 and reaching out to active editors who have recently approved articles for Wikipedia.18 Our participation group includes 3 editors with 1-5 years of experience, 4 with 6-10 years, and 3 with over 15 years of contribution. The study was approved by the Institutional Re- view Board of our institution and the participants signed the consent form through Qualtrics ques- tionnaires before the study started. To streamline the evaluation of grounded articles, we developed a web application, which features a side-by-side display of the article and its citation snippets, to gather ratings and open-ended feedback 15https://huggingface.co/mistralai/ Mistral-7B-Instruct-v0.1 16Following Gao et al. (2023), we check citation quality in the sentence level and split articles into sentences using NLTK sent_tokenize. sent_tokenize sometimes fails to split sen- tences correctly when the article contains special words like \u201cNo.12847\u201d, \u201cBhatia et al.\u201d, etc. 17https://meta.wikimedia.org 18Since evaluating Wikipedia-like articles is time- consuming and requires expertise, we paid each participant 50$ for our study. for each article. Figure 7 shows the screenshot of our web application and the full article produced by STORM is included in Table 12. For human evaluation, we use a 1 to 7 scale for more fine- grained evaluation. The grading rubric is included in Table 10. We collected the pairwise preferences and the perceived usefulness of STORM via an online ques- tionnaire. Specifically, for the perceived usefulness, we request editors to rate their agreement with state- ments \u201cI think it can be specifically helpful for my pre-writing stage (e.g., collecting relevant sources, outlining, drafting).\u201d, \u201cI think it will help me edit a Wikipedia article for a new topic\u201d, \u201cI think it can be a potentially useful tool for the Wikipedia community\u201d on",
        "chunk_index": 21,
        "start_char": 70223,
        "end_char": 74150
      },
      {
        "id": "7b81e331-2029-46f3-b2f1-265372d5574a",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "12. For human evaluation, we use a 1 to 7 scale for more fine- grained evaluation. The grading rubric is included in Table 10. We collected the pairwise preferences and the perceived usefulness of STORM via an online ques- tionnaire. Specifically, for the perceived usefulness, we request editors to rate their agreement with state- ments \u201cI think it can be specifically helpful for my pre-writing stage (e.g., collecting relevant sources, outlining, drafting).\u201d, \u201cI think it will help me edit a Wikipedia article for a new topic\u201d, \u201cI think it can be a potentially useful tool for the Wikipedia community\u201d on a Likert scale of 1-5, correspond- ing to Strongly disagree, Somewhat disagree, Nei- ther agree nor disagree, Somewhat agree, Strongly agree. E Error Analysis While articles produced by STORM are preferred by both automatic metrics and human evaluation, experienced editors still identified multiple prob- lems with the machine-generated articles. We an- alyze the free-form comments and summarize the major issues in Table 11. The primary issue raised is that the generated articles often contain emotional language and lack neutrality, primarily due to the source material. STORM currently retrieves grounding sources from the Internet which is not neutral and con- tains considerable promotional content on its own. Addressing this bias in the pre-writing stage repre- sents a valuable direction for future research. An- other major issue is the red herring fallacy or the over-association of unrelated facts. Addressing this challenge calls for high-level sensemaking rather than mere fact-level verification. Interest Level 1: Not engaging at all; no attempt to capture the reader\u2019s attention. 2: Slightly engaging with rare moments that capture attention. 3: Fairly engaging with a basic narrative but lacking depth. 4: Moderately engaging with several interesting points. 5: Quite engaging with a well-structured narrative and noteworthy points that frequently capture and retain attention. 6: Very engaging with a compelling narrative that captures and mostly retains attention. 7: Exceptionally engaging throughout, with a compelling narrative that consistently stimulates interest. Coherence and Organization 1: Disorganized; lacks logical structure and coherence. 2: Poor organization; some structure is evident but very weak. 3: Fairly organized; a basic structure is present but not consistently followed. 4: Organized; a clear structure is mostly followed with some lapses in coherence. 5: Good organization; a clear structure with minor lapses in coherence. 6: Very well-organized; a logical structure with transitions that effectively guide the reader. 7: Excellently organized; the article is logically structured with seamless transitions and a clear argument. Relevance and Focus 1: Off-topic; the content does not align with the headline or core subject. 2: Mostly off-topic with some relevant points. 3: Somewhat on topic but with several digressions; the core subject is evident but not consistently adhered to. 4: Generally on topic, despite a few unrelated details. 5: Mostly on topic and focused; the narrative has a consistent relevance to the core subject with infrequent digressions. 6: Highly relevant with a focused narrative and purpose. 7: Exceptionally focused and entirely on topic; the article is tightly centered on the subject, with every piece of information contributing to a comprehensive understanding of the topic. Broad Coverage 1: Severely lacking; offers little to no coverage of the topic\u2019s primary aspects, resulting in a very narrow perspective. 2: Minimal coverage; addresses only a small selection of the topic\u2019s main aspects, with significant omissions. 3: Partial coverage; includes some of the topic\u2019s main aspects but misses others, resulting in an incomplete portrayal. 4: Acceptable breadth; covers most main aspects, though it may stray into minor unnecessary details or overlook some relevant points. 5: Good coverage; achieves broad coverage",
        "chunk_index": 22,
        "start_char": 73542,
        "end_char": 77551
      },
      {
        "id": "7baa1009-d189-4149-b171-b650402aa8d6",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "is tightly centered on the subject, with every piece of information contributing to a comprehensive understanding of the topic. Broad Coverage 1: Severely lacking; offers little to no coverage of the topic\u2019s primary aspects, resulting in a very narrow perspective. 2: Minimal coverage; addresses only a small selection of the topic\u2019s main aspects, with significant omissions. 3: Partial coverage; includes some of the topic\u2019s main aspects but misses others, resulting in an incomplete portrayal. 4: Acceptable breadth; covers most main aspects, though it may stray into minor unnecessary details or overlook some relevant points. 5: Good coverage; achieves broad coverage of the topic, hitting on all major points with minimal extraneous information. 6: Comprehensive; provides thorough coverage of all significant aspects of the topic, with a well-balanced focus. 7: Exemplary in breadth; delivers outstanding coverage, thoroughly detailing all crucial aspects of the topic without including irrelevant information. Verifiability 1: No supporting evidence; claims are unsubstantiated. 2: Rarely supported with evidence; many claims are unsubstantiated. 3: Inconsistently verified; some claims are supported; evidence is occasionally provided. 4: Generally verified; claims are usually supported with evidence; however, there might be a few instances where verification is lacking 5: Well-supported; claims are very well supported with credible evidence, and instances of unsupported claims are rare. 6: Very well-supported; almost every claim is substantiated with credible evidence, showing a high level of thorough verification. 7: Exemplary verification; each claim is supported by robust, credible evidence from authoritative sources, reflecting strict adherence to the no original research policy. Table 10: Scoring rubrics on a 1-7 scale for human evaluation. Issue Mentioned Time Example Comments Use of emotional words, unneutral 12 The word \u201csignificant\u201d is used 17 times in this article. Vague and unsupported claims are made about broader political importance and \u201cpivotal role[s]\u201d, and is unencyclopedic. (comment on article Lahaina, Hawaii) [...] but they still have not fixed the issue of neutral point of view. It is also evident in this article that the writer\u2019s standpoint is biased towards Taylor Swift. Other than that, it did a good job at summarizing key points and putting depth into this. (comment on article Speak Now (Taylor\u2019s Version)) \u201cThe film was also featured in an art and film festival hosted by The California Endowment, highlighting the power of stories in reshaping narratives about communities.\u201d Yes, technically the source says that, but it\u2019s a stretch to say in Wikipedia voice and just sounds like non-neutral, promotional prose. (comment on article Gehraiyaan) Red herring fallacy, associating unrelated sources 11 Polling from America shouldn\u2019t be included and links to climate change shouldn\u2019t be made unless explicitly connected by the source. (comment on article Typhoon Hinnamnor) Sourcing seems mostly fine, though some aren\u2019t directly related (Ex. 39,40). (comment on article Gehraiyaan) Here is a lengthy digression about KISS, not necessary because the article on the band should be linked to. (comment on article 2022 AFL Grand Final) Missing important information 6 \u201cOne study, conducted by Sin\u00e9ad Griffin, a physicist at the Lawrence Berkeley National Laboratory, provided some analysis of LK-99\u2019s abilities using supercomputer simulations[20].\u201d This is not enough information about the analysis, which would have been very useful in the article. (comment on article LK-99) Although the earthquake\u2019s immediate aftermath and response are adequately covered, there could be more about the long-term socioeconomic impact and recovery processes. (comment on article 2022 West Java earthquake) Improper handling of time-sensitive information 5 Words like \u201cnow\u201d should be avoided in Wikipedia articles to prevent them from becoming dated and phrases such as, \u201cas of December 2023\u201d should be used instead. (comment on article Cyclone Batsirai)",
        "chunk_index": 23,
        "start_char": 76880,
        "end_char": 80969
      },
      {
        "id": "200e669d-8561-4bf7-b9e5-b5d8d3a7cc41",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "Berkeley National Laboratory, provided some analysis of LK-99\u2019s abilities using supercomputer simulations[20].\u201d This is not enough information about the analysis, which would have been very useful in the article. (comment on article LK-99) Although the earthquake\u2019s immediate aftermath and response are adequately covered, there could be more about the long-term socioeconomic impact and recovery processes. (comment on article 2022 West Java earthquake) Improper handling of time-sensitive information 5 Words like \u201cnow\u201d should be avoided in Wikipedia articles to prevent them from becoming dated and phrases such as, \u201cas of December 2023\u201d should be used instead. (comment on article Cyclone Batsirai) \u201cas of December 13\u201d doesn\u2019t specify a year, and is old information (comment on article 2022 West Java earthquake) Section organization problem 5 too many subsections in the \u201cRecovery and Rehabilitation\u201d section (comment on article 2022 West Java earthquake) I do not like how the article is organized, with too many headers cluttering the article, making it not as readable. Other than that, the AI did great work on the piece. (comment on article 2022 Crimean Bridge explosion) Table 11: Summary of major issues found on articles produced by STORM. Figure 7: Screenshot of the web application for evaluating the grounded article. Taylor Hawkins Oliver Taylor Hawkins (1972-2022) was an American musician, best known as the drummer for the rock band Foo Fighters[1]. Born in Fort Walton, Texas, Hawkins\u2019 love for music was ignited at a young age, particularly after watching a Queen concert in 1982[2][3][5]. He kick-started his professional career as the drummer for Alanis Morissette\u2019s band during her world tour for the hit album \u2019Jagged Little Pill\u2019[8][9]. His talents were recognized by Dave Grohl, who invited him to join the Foo Fighters in 1997, marking the beginning of his impactful tenure with the band[7][8]. Hawkins was celebrated for his versatile drumming style, drawing influence from renowned drummers like Roger Taylor, Neil Peart, Phil Collins, Alex Van Halen, and Stewart Copeland[14]. His performances, marked by a unique energy and aggressive style of play, earned him recognition as one of the top rock drummers of his era[15]. Apart from his role in the Foo Fighters, Hawkins\u2019 passion for music saw him involved in numerous side projects and collaborations, cementing his place in the world of rock music[10]. Outside of his professional life, Hawkins was known for his resilience and dedication to his family. Despite personal struggles, including a near-fatal drug overdose in 2001, Hawkins remained committed to his musical career[4][9]. His legacy continues to inspire musicians and fans alike, as his contributions to rock music, coupled with his indomitable spirit, made him an unforgettable icon in the music industry[13]. Hawkins\u2019 sudden death in 2022 while on tour in Bogot\u00e1, Colombia, sent shockwaves through the music world[34]. Tributes poured in from around the globe, reflecting the respect and admiration Hawkins had garnered during his lifetime[21][31]. His life and career were honored at a star-studded tribute concert in Los Angeles, attesting to the enduring impact of his music[22]. # Early Life and Background Oliver Taylor Hawkins, known as Taylor Hawkins, was born and raised in Fort Walton, Texas[3]. His family moved to Laguna Beach, California when he was four years old[3]. He has two younger siblings, a brother named Jason, and a sister named Heather[3]. As a child, Hawkins was particularly influenced by his paternal grandmother, Josie Hawkins, who had grown up during the Great Depression and lived in Jackson, Mississippi[1]. During his high school days at Laguna Beach High School, from where he graduated in 1990, he became friends with Jon Davison, who later became the lead vocalist of the band",
        "chunk_index": 24,
        "start_char": 80267,
        "end_char": 84126
      },
      {
        "id": "6c40cdb5-c3e3-4cb4-ade4-225250e23629",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "Background Oliver Taylor Hawkins, known as Taylor Hawkins, was born and raised in Fort Walton, Texas[3]. His family moved to Laguna Beach, California when he was four years old[3]. He has two younger siblings, a brother named Jason, and a sister named Heather[3]. As a child, Hawkins was particularly influenced by his paternal grandmother, Josie Hawkins, who had grown up during the Great Depression and lived in Jackson, Mississippi[1]. During his high school days at Laguna Beach High School, from where he graduated in 1990, he became friends with Jon Davison, who later became the lead vocalist of the band Yes[2][3]. His interest in music was nurtured from an early age, particularly after watching a Queen concert in 1982 which inspired him to learn to play the drums[2][5]. He noted that music was a constant presence in his family home[5]. Despite facing certain hardships during his upbringing, including his mother\u2019s struggles with \"demons\", Hawkins pursued his musical ambitions[4]. He credits his older sister Heather for taking care of the family during difficult times[4]. His first major musical experience came from playing drums for Alanis Morissette\u2019s album, Jagged Little Pill, and accompanying her on the subsequent tour[3]. This marked the beginning of his professional career in the music industry. # Career Taylor Hawkins began his professional music career playing in Alanis Morissette\u2019s band during her 18-month world tour in support of the hit album \u2019Jagged Little Pill\u2019 from 1995 to 1997[8][9]. His performances not only in the tour but also in the music videos for \u201cYou Oughta Know\u201d, \u201cAll I Really Want\u201d and \u201cYou Learn\u201d introduced him to the world of rock music and ultimately led to his meeting with Dave Grohl[8]. Throughout this time, Hawkins contributed significantly to the band\u2019s sound and performance, transforming the songs from their original drum loop format to a rock-band vibe that resonated with audiences[1][7]. In 1997, Hawkins was asked by Grohl to join the Foo Fighters, an invitation that he readily accepted[7][8]. At the time, Grohl thought it was a long shot to recruit Hawkins given that Morissette was at the height of her career, but Hawkins\u2019 desire to be a part of a rock band compelled him to make the move[7]. This marked the beginning of Hawkins\u2019 tenure as the drummer of the Foo Fighters, a role that he would play until his passing[6][9]. Apart from his work with Morissette and the Foo Fighters, Hawkins had an array of other musical experiences[10]. He drummed for Sass Jordan before joining Morissette\u2019s touring band[10]. He was part of an ad hoc drum supergroup called SOS Allstars and filled the void for Coheed and Cambria\u2019s 2007 album after their drummer Josh Eppard left the group[10]. In addition, Hawkins formed his own side project, the Coattail Riders, in 2005, through which he recorded his own music and took the project on the road, performing in small clubs despite the Foo Fighters\u2019 arena-status[7]. His son, Shane Hawkins, has since taken on his father\u2019s legacy, joining the Foo Fighters for a performance during the Boston Calling Music Festival in 2023[6]. # Musical Style and Influences Taylor Hawkins was a profound drummer, with his musical style and influences spreading across a wide array of rock genres[11]. Known for his passionate fandom of groups that came before him, Hawkins regularly expressed his admiration for bands like Rush, Genesis, and the Police, all of which featured some of the greatest drummers in rock history like Neil Peart, Phil Collins, and Stewart Copeland[11]. He was heavily influenced by his love for classic rock, as evidenced by his performances, where he covered songs from",
        "chunk_index": 25,
        "start_char": 83515,
        "end_char": 87204
      },
      {
        "id": "7bea3421-1b42-4f23-a632-d79c8c00af49",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "for a performance during the Boston Calling Music Festival in 2023[6]. # Musical Style and Influences Taylor Hawkins was a profound drummer, with his musical style and influences spreading across a wide array of rock genres[11]. Known for his passionate fandom of groups that came before him, Hawkins regularly expressed his admiration for bands like Rush, Genesis, and the Police, all of which featured some of the greatest drummers in rock history like Neil Peart, Phil Collins, and Stewart Copeland[11]. He was heavily influenced by his love for classic rock, as evidenced by his performances, where he covered songs from bands like Van Halen[11]. Hawkins drew influences from a variety of drumming styles, developing a signature style inspired by greats like Roger Taylor, Neil Peart, Phil Collins, Alex Van Halen, and Stewart Copeland[14]. This distinctive style and influence extended to his drum kit, which incorporated elements like rototoms and concert toms[14]. Beyond his influences, Hawkins had a unique energy that made him stand out as a drummer. His performances were recognized for their power, and he was known for his enthusiastic and aggressive style of play[15]. This earned him recognition as one of the top rock drummers of his time, with his passion for music living on through his performances[14]. Through his career, Hawkins left an indelible mark on rock music, through his distinct style, passion, and contributions to the music industry[13]. His love for music and dedication to his craft made him an unforgettable icon in the world of rock music[13]. # Personal Life Taylor Hawkins married Alison Hawkins, an American celebrity and entrepreneur, in 2005[18]. The couple had three children, Oliver, Annabelle, and Everleigh[19]. Hawkins\u2019 commitment to his family was evident; in fact, he even wrote a song for his middle child, Annabelle[9]. In his personal life, Hawkins had also struggled with drug use, which nearly claimed his life in a 2001 overdose[9][7][4]. However, he managed to overcome this challenge, and later expressed gratitude for the experience as a lesson that allowed him to realize the destructive path he was on[7]. Outside of his main role in the Foo Fighters, Hawkins also pursued various side projects including the Birds of Satan, NHC, and Chevy Metal. His motivation for such ventures was a constant drive to create and his love for music[7]. Hawkins was also known for his unabashed fanboy nature, often vocalizing his admiration for fellow musicians and his heroes[7]. # Legacy and Impact Taylor Hawkins was known for his raw and authentic drumming style, described as \"courageous, damaged and unflinchingly authentic\"[20]. His work with the Foo Fighters, as well as his various collaborations and side projects, made him a celebrated figure in rock \u2018n\u2019 roll[10]. Hawkins\u2019 death in 2022 was met with heartfelt tributes from colleagues and fans around the world. Notable tributes came from rock legends like Roger Taylor of Queen, who considered Hawkins as a kind, brilliant man and an inspirational mentor, likening his death to \"losing a younger favourite brother\"[21]. Similarly, Led Zeppelin\u2019s Jimmy Page admired his technique, energy and spirited enthusiasm[21]. An LA tribute concert held in his honor included guest drummers like Lars Ulrich of Metallica, Travis Barker of blink-182, and Brad Wilk of Rage Against the Machine. Singers like Miley Cyrus and Alanis Morissette also performed at the concert[22]. Apart from his music, Taylor Hawkins also contributed to charities Music Support and MusiCares, both of which were chosen by the Hawkins family[23]. He had received numerous accolades throughout his career, including 27 Grammy nominations, of which he won 14[2]. In 2021, the Foo Fighters were inducted into the Rock and Roll",
        "chunk_index": 26,
        "start_char": 86580,
        "end_char": 90377
      },
      {
        "id": "73dc5a6b-83ab-4d5a-8b28-b20f39af683d",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "Page admired his technique, energy and spirited enthusiasm[21]. An LA tribute concert held in his honor included guest drummers like Lars Ulrich of Metallica, Travis Barker of blink-182, and Brad Wilk of Rage Against the Machine. Singers like Miley Cyrus and Alanis Morissette also performed at the concert[22]. Apart from his music, Taylor Hawkins also contributed to charities Music Support and MusiCares, both of which were chosen by the Hawkins family[23]. He had received numerous accolades throughout his career, including 27 Grammy nominations, of which he won 14[2]. In 2021, the Foo Fighters were inducted into the Rock and Roll Hall of Fame[9]. # Discography Taylor Hawkins also led a notable music career through his own side projects and collaborations[10]. Aside from his work with the Foo Fighters, Hawkins formed and fronted the band Taylor Hawkins & The Coattail Riders, a project which originated from jamming sessions with his friend Drew Hester[10]. ### Taylor Hawkins & The Coattail Riders Taylor Hawkins & The Coattail Riders, a band formed in 2004, have released three albums and their music spans genres including Hard Rock, Art Rock, and Alternative Rock[24][25][26]. The band grew from an initial casual jamming session, gradually evolving into a more formal arrangement that led to the production of record albums. Notably, these albums featured guest appearances by renowned musicians such as Dave Grohl, Queen\u2019s Brian May and Roger Taylor, The Cars\u2019 Elliot Easton, Perry Farrell, and Jon Davison, who is a school friend of Hawkins\u2019[10]. ### Red Light Fever Red Light Fever, released on April 19, 2010, was the band\u2019s first album[29][30]. Prior to its release, Hawkins revealed in an interview that the album had completed the recording and production stages, but its title and release date were yet to be determined[29]. Red Light Fever was recorded at the Foo Fighters\u2019 Studio 606 in California and featured guest musicians such as Brian May and Roger Taylor of Queen, Dave Grohl of Foo Fighters, and Elliot Easton of The Cars[29][30]. ## Get the Money Get the Money, the third album from Taylor Hawkins & The Coattail Riders, was released on November 8, 2019[29]. The album\u2019s first single, \"Crossed the Line\", released on October 15, 2019, featured Dave Grohl and Jon Davison, the frontman of Yes[29]. The music video for the single \"I Really Blew It\" also featured appearances from Grohl and Perry Farrell[29]. # Collaborations and Guest Appearances Throughout his career, Taylor Hawkins collaborated with various prominent artists and bands. The Coattail Riders\u2019 albums notably featured appearances from luminaries such as Brian May and Roger Taylor of Queen, Chrissie Hynde, Nancy Wilson of Heart, Sex Pistol Steve Jones and James Gang\u2019s Joe Walsh[28]. Hawkins also fronted another group, The Birds of Satan, which evolved from his heavy rock covers band, Chevy Metal[28]. Despite his diverse musical engagements, Hawkins always maintained a close allegiance with the Foo Fighters, which remained the center of his music life[7][28]. # Tragic Passing Taylor Hawkins, the esteemed drummer of the alt-rock band Foo Fighters, passed away suddenly on March 25, 2022, while on tour with his band in Bogot\u00e1, Colombia[34]. The official cause of death was cardiac arrest, though inquiries were raised concerning the presence of drugs in his system and their potential contribution to his death[33][34]. On the night of his passing, paramedics were called to the Four Seasons hotel in Bogot\u00e1 due to reports of chest pain from an unnamed guest, later revealed to be Hawkins[34]. Unfortunately, resuscitation efforts were unsuccessful, and Hawkins was declared dead at the scene[34]. The news of Hawkins\u2019 sudden demise was announced on the morning of March 25th,",
        "chunk_index": 27,
        "start_char": 89740,
        "end_char": 93524
      },
      {
        "id": "8860454b-99aa-43c0-afd0-17281bf986d0",
        "document_id": "89fbc0f3-8095-497e-b9f7-b6c8de4e6b63",
        "content": "away suddenly on March 25, 2022, while on tour with his band in Bogot\u00e1, Colombia[34]. The official cause of death was cardiac arrest, though inquiries were raised concerning the presence of drugs in his system and their potential contribution to his death[33][34]. On the night of his passing, paramedics were called to the Four Seasons hotel in Bogot\u00e1 due to reports of chest pain from an unnamed guest, later revealed to be Hawkins[34]. Unfortunately, resuscitation efforts were unsuccessful, and Hawkins was declared dead at the scene[34]. The news of Hawkins\u2019 sudden demise was announced on the morning of March 25th, 2022, which left the music world in shock[32]. The band confirmed the news with a short statement, expressing their devastation at the loss of Hawkins, whose \"musical spirit and infectious laughter\" would live on forever[32]. As a result of Hawkins\u2019 untimely passing, the band canceled their ongoing South American tour[33]. The festival stage at the Est\u00e9reo Picnic Festival, where the Foo Fighters were scheduled to perform that night, was transformed into a candlelight vigil in memory of Hawkins[33]. ## Tributes and Remembrances In the wake of Hawkins\u2019 death, tributes from fans and colleagues alike poured in from around the world[21][31]. Among the many paying their respects were legendary rock and roll musicians like Roger Taylor, the drummer of Queen, who Hawkins credited with inspiring his own career behind the drum set[21]. In heartfelt social media posts, Taylor described Hawkins as an \"inspirational mentor\" and a \"kind brilliant man\"[21], while Led Zeppelin\u2019s Jimmy Page reminisced about sharing the stage with Hawkins and praised his \"technique, energy and spirited enthusiasm\"[21]. There were also numerous onstage tributes to Hawkins. Notably, Miley Cyrus expressed her grief and sent peaceful wishes to the Foo Fighters and the Hawkins family during a performance at Lollapalooza[31]. Similarly, Liam Gallagher of Oasis dedicated one of the band\u2019s biggest hits to Hawkins during a concert at the Royal Albert Hall in London[31]. Fans gathered outside the hotel where Hawkins died, lighting candles, leaving flowers, and singing the band\u2019s songs in his honor[31]. Hawkins\u2019 life and career were celebrated in a star-studded tribute concert in Los Angeles, which saw performances from over 50 musicians, including his former bands and colleagues from Def Leppard, Queen, and Foo Fighters[22]. Table 12: STORM\u2019s generated article for \u201cTaylor Hawkins\u201d. \u201c#\u201d, \u201c##\u201d indicate the section title and subsection title respectively. Numbers in brackets indicate the cited references.",
        "chunk_index": 28,
        "start_char": 92903,
        "end_char": 95517
      }
    ],
    "created_at": "2025-08-06 09:16:17.188161",
    "file_size": 1320410,
    "source_type": "pdf",
    "original_url": null
  },
  "0127506d-5f2d-45c3-8eb1-8b00ccb885a0": {
    "id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
    "name": "Theory_of_Mind_Prediction.pdf",
    "file_path": "/Users/will/AVI BUILD/Able3_Main_WithVoiceMode/data/sources/Theory_of_Mind_Prediction.pdf",
    "summary": "Boosting Theory-of-Mind Performance in Large Language Models via Prompting Shima Rahimi Moghaddam*, Christopher J. Honey Johns Hopkins University, Baltimore, MD, USA. * Correspondence to: sh.rahimi.m@gmail.com Abstract Large language models (LLMs) excel in many tasks in 2023, but they still face ...",
    "chunks": [
      {
        "id": "f1027130-cd5c-4dfc-964b-3b7d561a0e4f",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "Boosting Theory-of-Mind Performance in Large Language Models via Prompting Shima Rahimi Moghaddam*, Christopher J. Honey Johns Hopkins University, Baltimore, MD, USA. * Correspondence to: sh.rahimi.m@gmail.com Abstract Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT- 3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM accuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate prompting enhances LLM ToM reasoning, and they underscore the context-dependent nature of LLM cognitive capacities. Introduction What kinds of reasoning can large language models (LLMs) perform about everyday scenarios? Large language models (LLMs) have shown great success in a variety of tasks; however, they still struggle with tasks that require reasoning (Mahowald et al., 2023; Rae et al., 2021). One area of specific interest the is so-called \u201ctheory of mind\u201d (ToM) reasoning, which involves tracking the mental state of agents, such as their goals, and what they know (Kosinski, 2023; Langley et al., 2022). Language models have dramatically advanced in the range of everyday questions to which they can accurately respond, but their ToM performance is thought to be relatively poor (Cuzzolin et al., 2020; Sap et al., 2022; Ullman, 2023). Here, we test the hypothesis that appropriate prompting can enhance the ToM performance of LLMs. The capacity of LLMs to reliably perform ToM reasoning is important for several reasons. First, ToM is an essential element of social understanding, allowing people to participate in intricate social exchanges and to anticipate the actions or responses of others (Bedny et al., 2009; Heyes and Frith, 2014; Kidd and Castano, 2013; Moran et al., 2011; Seyfarth and Cheney, 2013; Young et al., 2007). Second, ToM is considered a complex cognitive capacity which is most highly developed in humans, and a small number of other animals (Krupenye and Call, 2019; Povinelli and Preuss, 1995). This may be because ToM relies on structured relational knowledge (e.g. agents can have goals; and agent X has goal G, but agent Y does not know that agent X has goal G). Models that work with social information and with humans will benefit from being able to reason about the mental states and beliefs of agents. Finally, ToM tasks often involve inferential reasoning. For instance, for successful ToM performance, LLMs need to reason based on unobservable information (e.g. hidden mental states of agents) that must be inferred from context rather than parsed from the surface text (e.g. explicitly stated features of a situation). Hence, assessing and improving these models' proficiency in ToM tasks could offer valuable insights into their potential for a wider range of tasks that require inferential reasoning. In-context learning approaches can enhance the reasoning capacity of LLMs. Brown et al. (Brown et al., 2020) showed that, for sufficiently large language models (+100B parameters), one can enhance models\u2019 performance using only few-shot task demonstrations that are specified only through the input to",
        "chunk_index": 0,
        "start_char": 0,
        "end_char": 4006
      },
      {
        "id": "6ad618a3-f21b-4738-acb9-b028c6aba7f7",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "on unobservable information (e.g. hidden mental states of agents) that must be inferred from context rather than parsed from the surface text (e.g. explicitly stated features of a situation). Hence, assessing and improving these models' proficiency in ToM tasks could offer valuable insights into their potential for a wider range of tasks that require inferential reasoning. In-context learning approaches can enhance the reasoning capacity of LLMs. Brown et al. (Brown et al., 2020) showed that, for sufficiently large language models (+100B parameters), one can enhance models\u2019 performance using only few-shot task demonstrations that are specified only through the input to the model (i.e. at inference time, without weight updates). This form of performance boosting is usually referred to as \u201cfew-shot learning\u201d. Wei et al. (Wei et al., 2022) later showed that the ability of LLMs to perform complex reasoning was improved when the few-shot examples in the prompt contain the reasoning steps for reaching a conclusion (\u201cchain-of- thought reasoning\u201d) (Magister et al., 2022). Moreover, Kojima et al. (Kojima et al., 2022) showed that, even in the absence of exemplar demonstrations, instructing language models to think \u201cstep-by-step\u201d enhances their reasoning performance. There is not currently a theoretical understanding of why these prompting techniques are beneficial, however some recent studies have explored the effects of compositional structure and local dependencies in training data on efficacy of these methods (Hahn and Goyal, 2023; Prystawski and Goodman, 2023). The capability of LLMs to perform ToM reasoning is supported by some studies (Bubeck et al., 2023; Kosinski, 2023), but questioned by others (Sap et al., 2022; Trott et al., 2022; Ullman, 2023). Though this prior literature provides many insights into ToM in LLMs, the quantitative evaluations of ToM performance have two main limitations. First, they examine LLMs\u2019 ToM performance only on single-word or multiple-option completion (Kosinski, 2023; Sap et al., 2022; Trott et al., 2022; Ullman, 2023). However, LLMs may benefit from freely producing answers with multiple parts and speculating over multiple possibilities, rather than being assessed on a single word completion. Second, most of the work criticizing the ToM abilities of LLMs relied on either zero- shot testing (Trott et al., 2022; Ullman, 2023) or provided examples that lacked step-by-step reasoning toward an answer (Sap et al., 2022). Yet, the type of output generated by LLMs can be highly context-sensitive (Sejnowski, 2023). Therefore, we asked whether recent LLMs might exhibit improved ToM performance when provided with suitable prompts. Here we evaluate the performance of LLMs faced with ToM comprehension questions and we explore whether this performance can be boosted using prompting methods such as step-by-step thinking, few-shot learning, and chain-of-thought reasoning (Brown et al., 2020; Kojima et al., 2022; Wei et al., 2022). Improving inferential reasoning performance by prompting is important because it is a flexible approach that does not require additional training or large new datasets. Further, if effective prompting techniques guide LLMs towards generating higher-quality ToM responses, this contributes to the overall reliability of their reasoning in wide- ranging everyday applications. Methods Models We studied the four most recent GPT models from the Open AI family. These were GPT-4 (OpenAI, 2023a) as well as the Davinci-2, Davinci-3, and GPT-3.5-Turbo models, which are considered GPT-3.5 variants that improve on GPT-3 (Brown et al., 2020; Ouyang et al., 2022). These are all large models (+100B parameters), but they differ in their training methods (OpenAI, 2023c). Davinci-2 (API name: text-davinci-002) is one of the GPT-3.5 models which (in addition to the GPT-3 curricula) was also trained with supervised fine-tuning on human-written demonstrations (OpenAI, 2023b; Stiennon et al., 2020). Davinci-3 (API name: text-davinci-003), another GPT-3.5 model,",
        "chunk_index": 1,
        "start_char": 3329,
        "end_char": 7381
      },
      {
        "id": "9670bbdb-2261-4972-b9e5-a924b8c6cbf0",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "Models We studied the four most recent GPT models from the Open AI family. These were GPT-4 (OpenAI, 2023a) as well as the Davinci-2, Davinci-3, and GPT-3.5-Turbo models, which are considered GPT-3.5 variants that improve on GPT-3 (Brown et al., 2020; Ouyang et al., 2022). These are all large models (+100B parameters), but they differ in their training methods (OpenAI, 2023c). Davinci-2 (API name: text-davinci-002) is one of the GPT-3.5 models which (in addition to the GPT-3 curricula) was also trained with supervised fine-tuning on human-written demonstrations (OpenAI, 2023b; Stiennon et al., 2020). Davinci-3 (API name: text-davinci-003), another GPT-3.5 model, is an upgraded version of Davinci-2 which was further trained with Reinforcement Learning from Human Feedback (RLHF) using Proximal Policy Optimization (OpenAI, 2023b; Ouyang et al., 2022; Stiennon et al., 2020). GPT-3.5-Turbo (the original version of ChatGPT) (OpenAI, 2023b) is yet another GPT-3.5 model, trained with both fine-tuning on human-written demonstrations and RLHF, then further optimized for conversation. GPT-4 is the most recent GPT model as of April 2023 (OpenAI, 2023a); there are few published details of the size and training methods for GPT-4, however, it appears to have undergone more intensive training with RLHF for better alignment with human intention (OpenAI, 2023a). We tested all models in a setting with temperature equal to 0.4 and the maximum length of generated text set to 150 tokens. Experimental Design To examine the comprehension ability of these models on ToM scenarios, we evaluated their comprehension accuracy on both ToM scenarios and Control scenarios. The Control scenarios describe a scene (\u201cPhoto\u201d) without any agents. We refer to them as Photo scenarios (Supplement A). The ToM scenarios describe the mental state of people involved in a situation. We adapted 16 Photo scenarios and 16 ToM scenarios from stimulus sets used in human fMRI to localize the brain areas involved in ToM (Dodell-Feder et al., 2011) (Supplement B). These scenarios match in their general difficulty; however, they differ in the need to reason based on individuals\u2019 state of mind in the scenario. Human participants showed the same level of accuracy on both types of scenarios in prior studies (Dodell-Feder et al., 2011), as well as in our behavioral experiment. In our human experiments, participants were given 18 seconds to read each scenario. They were then asked a comprehension question about the scenario on a new screen and could answer at their own pace by clicking Yes or No. The Photo and ToM scenarios were mixed and presented in random order so that each participant experienced both types of scenarios. Participants showed similar accuracy across Photo (86% \u00b14%) and ToM (87% \u00b14%) scenarios (see Supplement C). A sample Photo Scenario (Control): Scenario: \"A map shows the ground floor plan. A photocopy was sent to the architect yesterday, but at the time the kitchen door was missing. It was added to the map this morning.\" Question: Does the architect\u2019s photocopy show the door? A sample ToM Scenario: Scenario: \"The morning of the high school dance Sarah placed her high heel shoes under her dress and then went shopping. That afternoon, her sister borrowed the shoes and later put them under Sarah's bed.\" Question: When Sarah gets ready, does she assume her shoes are under her dress? Dependent Variable We measured the models\u2019 accuracy in answering comprehension questions following the brief scenario. Since LLMs are probabilistic models, we ran each prompt 20 times for every model, restarting the models for every repetition to ensure that the performance history of the models was not included, and to measure the reliability of the model\u2019s answers across re-initialization. Accuracy was defined",
        "chunk_index": 2,
        "start_char": 6711,
        "end_char": 10528
      },
      {
        "id": "072c69ee-2775-4f5b-9366-712b0c1b00d6",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "her high heel shoes under her dress and then went shopping. That afternoon, her sister borrowed the shoes and later put them under Sarah's bed.\" Question: When Sarah gets ready, does she assume her shoes are under her dress? Dependent Variable We measured the models\u2019 accuracy in answering comprehension questions following the brief scenario. Since LLMs are probabilistic models, we ran each prompt 20 times for every model, restarting the models for every repetition to ensure that the performance history of the models was not included, and to measure the reliability of the model\u2019s answers across re-initialization. Accuracy was defined as the proportion of correct responses across 320 questions (16 scenarios x 20 repetitions). To evaluate the performance of each model, we analyzed their response to each prompt. To measure the accuracy of each model, we manually checked all the LLM-generated responses and labeled them as correct or incorrect. Due to the complexity of natural language, it was not always possible to label model output as correct or incorrect based on match to a template answer (e.g. \u201cYes\u201d or \u201cNo\u201d). We labeled an answer as correct if it had the correct conclusion (see Examples 1-5 in \u201cCorrect responses\u201d below) or if the model included the correct reasoning as one possibility amongst multiple reasonable responses (see Example 6 in \u201cCorrect responses\u201d below). We labeled responses as incorrect if they arrived at an incorrect overall conclusion (see Examples 1-4 in \u201cIncorrect responses\u201d below) or if the responses were inconclusive (see Examples 5 and 6 in \u201cIncorrect responses\u201d below). Below are the examples of correct and incorrect responses to a ToM comprehension question: Scenario: \"The morning of the high school dance Sarah placed her high heel shoes under her dress and then went shopping. That afternoon, her sister borrowed the shoes and later put them under Sarah's bed.\" Question: When Sarah gets ready, does she assume her shoes are under her dress? Correct Responses: - Example 1: Sarah placed her shoes under her dress before she went shopping, but her sister borrowed them and put them under Sarah's bed. Sarah doesn't know that her sister borrowed her shoes, so she may assume that they are still under her dress. - Example 2: Sarah placed her high heel shoes under her dress before going shopping, but her sister borrowed the shoes and put them under Sarah's bed later. So, Sarah doesn't know that her sister borrowed and returned the shoes. Therefore, the answer is: Yes, she assumes her shoes are under her dress. - Example 3: Sarah may assume her shoes are still under her dress because that's where she placed them in the morning. - Example 4: Sarah placed her high heel shoes under her dress before she went shopping, so she knows where she left them. Her sister borrowed the shoes and later put them under Sarah's bed, but Sarah doesn't know that yet. So, the answer is: Yes, Sarah assumes her shoes are under her dress unless she discovers otherwise. - Example 5: In the morning, Sarah placed her high heel shoes under her dress. Later, her sister borrowed the shoes and put them under Sarah's bed. Therefore, when Sarah gets ready, she may assume her shoes are still under her dress, but they are actually under her bed. - Example 6: Based on this scenario, it is unclear whether Sarah assumes her shoes are under her dress when she gets ready. It depends on whether her sister told her she borrowed them and put them under the bed. Incorrect Responses: - Example 1: Sarah placed her shoes under her dress",
        "chunk_index": 3,
        "start_char": 9888,
        "end_char": 13451
      },
      {
        "id": "1ede9529-d7f5-46a7-8698-bafb0539b287",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "Example 5: In the morning, Sarah placed her high heel shoes under her dress. Later, her sister borrowed the shoes and put them under Sarah's bed. Therefore, when Sarah gets ready, she may assume her shoes are still under her dress, but they are actually under her bed. - Example 6: Based on this scenario, it is unclear whether Sarah assumes her shoes are under her dress when she gets ready. It depends on whether her sister told her she borrowed them and put them under the bed. Incorrect Responses: - Example 1: Sarah placed her shoes under her dress in the morning and then went shopping. Her sister borrowed the shoes and put them under Sarah's bed. So, Sarah doesn't know that her shoes are under her bed. So, the answer is: No, Sarah doesn't assume her shoes are under her dress. - Example 2: No, Sarah does not assume her shoes are under her dress. - Example 3: First, Sarah placed her high heel shoes under her dress in the morning. Then, her sister borrowed the shoes and later put them under Sarah's bed. Therefore, Sarah would not assume her shoes are under her dress as they were moved by her sister. - Example 4: In the morning, Sarah placed her high heel shoes under her dress, but then went shopping. This means that when she returned home, she did not check if her shoes were still under her dress. Later, her sister borrowed the shoes and put them under Sarah's bed. Therefore, Sarah cannot assume that her shoes are under her dress as they were moved by her sister. - Example 5: It does not specify in the scenario whether or not Sarah assumes her shoes are under her dress. - Example 6: Sarah placed her high heel shoes under her dress in the morning. She then went shopping. Her sister later borrowed the shoes and put them under Sarah's bed. Based on this information, it is unclear if Sarah assumes her shoes are still under her dress when she gets ready for the dance. Experimental Conditions To measure the efficacy of in-context learning (ICL) on ToM performance, we examined each scenario with four types of prompting: (1) Zero-shot (no ICL); (2) Zero-shot with step-by-step (SS) thinking; (3) Two-shot chain-of-thought (CoT) reasoning; (4) Two-shot CoT reasoning with SS thinking. Below are the examples of each prompting method (Figure 1). Figure 1. Demonstration of Prompting Methods used for Boosting ToM reasoning in LLMs. Examples of 4 prompting types used to test the ToM performance of LLMs. Each box provides an example of the input to the model for a single trial in one condition. For each trial, all of the text shown after the word \u201cPrompt:\u201d was input to the model, including the final text line beginning with \u201cA:\u201d. Results Zero-shot Performance We first compared the models\u2019 zero-shot performance on Photo and ToM scenarios. We found that accuracy in Photo scenarios gradually increased with the recency of the models, with lowest performance in Davinci-2 and highest in GPT-4 (Figure 2 A). However, in contrast to Photo comprehension, the accuracy on ToM questions did not monotonically improve with the recency of the models: Davinci-2 was more accurate than Davinci-3, which was in turn more accurate than GPT-3.5-Turbo (Figure 2 B). Although the lower zero-shot ToM accuracy in GPT-3.5-Turbo in comparison to Davinci-2 may seem to imply that the Turbo model\u2019s reasoning performance is inferior, the primary reason for its lower accuracy was its tendency to provide an inconclusive response. Specifically, it would often state that there was insufficient information",
        "chunk_index": 4,
        "start_char": 12898,
        "end_char": 16409
      },
      {
        "id": "64821a75-a14b-4b69-8784-510ba33d2438",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "models, with lowest performance in Davinci-2 and highest in GPT-4 (Figure 2 A). However, in contrast to Photo comprehension, the accuracy on ToM questions did not monotonically improve with the recency of the models: Davinci-2 was more accurate than Davinci-3, which was in turn more accurate than GPT-3.5-Turbo (Figure 2 B). Although the lower zero-shot ToM accuracy in GPT-3.5-Turbo in comparison to Davinci-2 may seem to imply that the Turbo model\u2019s reasoning performance is inferior, the primary reason for its lower accuracy was its tendency to provide an inconclusive response. Specifically, it would often state that there was insufficient information to determine the answer to the question (see examples 5-6 in \u201cIncorrect responses\u201d in Dependent Variable). However, more recent models were not always more equivocal in ToM responding: GPT-4 demonstrated a significantly greater ToM accuracy than all other models (independent t-test between GPT-4 and Davinci-2: \uf044Acc = 0.11, p-value <0.001) (Figure 2B). Overall, GPT-4 showed the best zero-shot performance in both Photo and ToM scenarios (Figure 2 A and B). Figure 2. Zero-Shot Accuracy of LLMs in Photo (Control) and ToM Comprehension Questions. A) Zero-shot accuracy of LLMs in Photo scenarios (non-agentive) comprehension questions. B) Zero-shot accuracy of LLMs in ToM comprehension questions. The values show the mean accuracy for each model averaged over 320 measurements, as there are 16 scenarios of each type, and each scenario was tested 20 times with re-initialization. For plotting the error bars, we treated each \u201crepetition\u201d as if it were a single \u201cparticipant\u201d in the experiment. Therefore, we have 20 accuracy values, each averaged across 16 values from different ToM questions. The error bars then show the standard deviation of the 20 mean-accuracy values. Performance when Supported by Prompting In-context learning via modified prompting boosted the ToM performance of all GPT models that were released after Davinci-2 (Figure 3). First, we instructed the models to think step by step (SS). We found that SS thinking enhanced the performance of Davinci-3, GPT-3.5-Turbo, and GPT-4 (e.g. independent t-test for zero-shot vs SS thinking for GPT-4: \uf044Acc = 0.10, p-value <0.001; zero-shot vs SS thinking for GPT-3.5-Turbo: \uf044Acc = 0.06, p-value <0.001). However, SS thinking did not improve the accuracy of Davinci-2 (Figure 3). Figure 3. Effects of In-context Learning Prompts on ToM performance in LLMs. ToM performance of models using various in-context learning methods. For each model, the gray bar on the far left shows the Zero-Shot baseline ToM performance. The next three bars (orange) show the ToM performance on Zero- Shot plus SS Thinking; Two-Shot CoT; and Two-Shot CoT plus SS Thinking. Error bars indicate the standard deviation across 20 repetitions (see Figure 2, caption). Second, we tested prompting with Two-shot chain-of-thought (CoT) reasoning. We found that Two-shot CoT increased the accuracy of all models that were trained with RLHF (all models except Davinci-2) (Figure 3). For GPT-3.5-Turbo, Two-shot CoT prompting significantly improved the performance above its zero-shot baseline (\uf044Acc = 0.25, p-value <0.001), and was significantly more effective than prompting with SS thinking (\uf044Acc = 0.19, p-value <0.001). For Davinci-3 and GPT-4, prompting with Two-shot CoT was slightly more effective than instructing step-by-step thinking. Joint prompting with both Two-shot CoT reasoning and SS thinking produced the greatest increase in models\u2019 accuracy (Figure 3). ToM accuracy of all RLHF-trained models was significantly increased when the prompts included Two-shot CoT reasoning and SS thinking: Accuracy of Davinci-3 was increased by 20% \u00b1 6% (mean \u00b1 std) relative to its zero-shot baseline (p-value <0.001). Accuracy of GPT-3.5-Turbo was increased by 41% \u00b1 5% relative to its zero-shot performance (p-value <0.001). Finally, accuracy of GPT-4 increased by 21%",
        "chunk_index": 5,
        "start_char": 15751,
        "end_char": 19716
      },
      {
        "id": "b6cc663d-9835-4bf2-96e1-711051db7a1a",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "= 0.19, p-value <0.001). For Davinci-3 and GPT-4, prompting with Two-shot CoT was slightly more effective than instructing step-by-step thinking. Joint prompting with both Two-shot CoT reasoning and SS thinking produced the greatest increase in models\u2019 accuracy (Figure 3). ToM accuracy of all RLHF-trained models was significantly increased when the prompts included Two-shot CoT reasoning and SS thinking: Accuracy of Davinci-3 was increased by 20% \u00b1 6% (mean \u00b1 std) relative to its zero-shot baseline (p-value <0.001). Accuracy of GPT-3.5-Turbo was increased by 41% \u00b1 5% relative to its zero-shot performance (p-value <0.001). Finally, accuracy of GPT-4 increased by 21% to reach 100% accuracy (p-value <0.001, ceiling performance). Altogether, appropriate prompting enabled all RLHF-trained models to achieve accuracy greater than 80%. When appropriately prompted, Davinci-3 achieved ToM accuracy of 83% (\u00b16%), GPT-3.5-Turbo achieved 91% (\u00b15%), and GPT-4 reached ceiling accuracy of 100%. Human performance in these scenarios was 87% (\u00b14%) (See Supplement C). Interim Discussion Do the increases in ToM performance arise from copying the reasoning steps from the prompt? The improved performance of LLMs via prompting does not appear to be due to simple mimicry of the specific reasoning steps provided in the chain-of-thought examples. The logic of reasoning in some scenarios was different from the reasoning logic of the 2 chain-of-thought examples. Both the in-context examples had the following essential reasoning logic: Person P was not at location L when event E happened, so they are not aware of event E. Conversely, some of the scenarios required reasoning that event E happened when person P was not there, but when P arrives, they can see the result of event E. If the improved performance was only due to copying a specific sort of reasoning it should not generalize across these distinct reasoning cases. To underscore this point, we performed the following analyses to test whether closely-related vs distantly-related CoT examples produce similar performance increases. We hypothesized that if the improved ToM performance is due to copying the reasoning steps from the in- context ToM examples, then prompting with non-ToM examples should not enhance the ToM performance. To test this hypothesis, it is necessary to exclude the ToM questions for which the models consistently provided accurate zero-shot answers, because for such scenarios it is not possible to measure a performance increase. In other words, we focused on the ToM questions that the models could not correctly answer in zero-shot. Furthermore, we focused this analysis on Davinci-3 and GPT-3, because these models benefited from in-context chain-of-thought ToM examples and (in contrast to GPT-3.5-Turbo) they almost always returned a definitive response which did not require any subjective interpretation. Then for each model, we selected the scenarios that they could not correctly answer in zero-shot. This resulted in 4 scenarios for GPT-4 (mean zero-shot accuracy of 0.16), and 6 scenarios in Davinci-3 (zero-shot accuracy of 0.0). We then tested the ToM accuracy of the model for these selected scenarios under the following conditions: (i) Two-shot ToM CoT examples; (ii) Two-shot Non-ToM Inferential CoT examples; and (iii) Two-shot Photo CoT examples (Non-ToM Inferential examples; and Photo examples are shown in Supplement D). In the Non-ToM Inferential examples, questions were not about false belief or an agent\u2019s state of mind, but rather required inferential reasoning about the consequences of an event. Photo examples were selected from scenarios that described a scene or situation without an agent. The Davinci-3 and GPT-4 models experienced increases in ToM performance from all of the classes of CoT examples that we tested: Photo examples, Non-ToM Inferential examples, and ToM examples. The mean accuracy increases for each model and each",
        "chunk_index": 6,
        "start_char": 19043,
        "end_char": 22995
      },
      {
        "id": "bbc3519a-645d-4f1f-bfee-c132afafcb79",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "CoT examples; and (iii) Two-shot Photo CoT examples (Non-ToM Inferential examples; and Photo examples are shown in Supplement D). In the Non-ToM Inferential examples, questions were not about false belief or an agent\u2019s state of mind, but rather required inferential reasoning about the consequences of an event. Photo examples were selected from scenarios that described a scene or situation without an agent. The Davinci-3 and GPT-4 models experienced increases in ToM performance from all of the classes of CoT examples that we tested: Photo examples, Non-ToM Inferential examples, and ToM examples. The mean accuracy increases for each model and each type of CoT example are shown in Figure 4, while the accuracy changes for individual ToM questions are shown in Figure S.1. Prompting with Inferential and Photo examples boosted the models\u2019 performance on ToM scenarios even though these in-context examples did not follow the same reasoning pattern as the ToM scenarios. Therefore, our analysis suggests that the benefit of prompting for boosting ToM performance is not due to merely overfitting to the specific set of reasoning steps shown in the CoT examples. Instead, the CoT examples appear to invoke a mode of output that involves step-by-step reasoning, which improves the accuracy across a range of tasks. Figure 4. Effects of Various Types of CoT Examples on ToM Performance. The Y-axis represents the change in ToM accuracy compared to zero-shot (Two-shot CoT minus zero-shot), averaged across the scenarios that each model answered incorrectly during zero-shot testing (4 scenarios for GPT-4 and 6 scenarios for Davinci-3). The X-axis displays the types of in-context CoT examples provided to the model for evaluating ToM performance. The values indicate the mean change in accuracy compared to zero-shot for the assessed scenarios. For GPT-4, error bars are the standard deviation across the 4 accuracy-change values (1 value per scenario). For Davinci-3, error bars are the standard deviation across the 6 accuracy- change values. Refer to Figure S.1 for per-scenario accuracy results. General Discussion We studied the performance of LLMs on comprehension tasks thought to require reasoning about an individual\u2019s state of mind. We evaluated the performance of Davinci-2, Davinci-3, GPT-3.5-Turbo, and GPT-4. When comprehension tasks required reasoning about superficially observable information in brief text scenarios (Photo scenarios), the zero-shot comprehension accuracy of LLMs increased monotonically from the oldest to newest LLMs. At the same time, the zero-shot accuracy on ToM questions did not consistently improve with the recency and sophistication of the models. However, with appropriate prompting, more recent models did exhibit consistent improvements in ToM performance. The prompt-improved performance approached or, in the case of GPT-4, exceeded human ToM performance on our test set. Prompts that allowed for in-context learning boosted the expression of ToM reasoning in LLMs, relative to the zero-shot baseline. We used two recent prompting methods: step-by-step thinking (Kojima et al., 2022) and chain-of-thought reasoning (Wei et al., 2022). We found that all models, except Davinci-2, were able to exploit the modified prompting for higher ToM accuracy. The models exhibited the greatest increase in accuracy when the prompts combined both chain of thought reasoning and step-by-step thinking, rather than employing either alone. Also, the contrast between the performance of GPT-3.5-Trubo in zero-shot conditions and ICL conditions is a reminder that measured performance may not always reflect competence (Firestone, 2020). When provided with appropriate prompting, GPT-3.5-Turbo was capable of achieving higher performance than Davinci-2. The GPT-3.5 variants that we tested (Davinci-2, Davinci-3, and GPT-3.5-Turbo) are similar in size (~175 B parameters) but differ in their training. In particular, Davinci-2 was the only model that was not finetuned with RLHF, and it was also",
        "chunk_index": 7,
        "start_char": 22342,
        "end_char": 26370
      },
      {
        "id": "b307dfe3-85dc-4b92-b97e-69ac0fa938fd",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "increase in accuracy when the prompts combined both chain of thought reasoning and step-by-step thinking, rather than employing either alone. Also, the contrast between the performance of GPT-3.5-Trubo in zero-shot conditions and ICL conditions is a reminder that measured performance may not always reflect competence (Firestone, 2020). When provided with appropriate prompting, GPT-3.5-Turbo was capable of achieving higher performance than Davinci-2. The GPT-3.5 variants that we tested (Davinci-2, Davinci-3, and GPT-3.5-Turbo) are similar in size (~175 B parameters) but differ in their training. In particular, Davinci-2 was the only model that was not finetuned with RLHF, and it was also the only model whose ToM performance was not increased by our prompt manipulations. It is possible that the RLHF component of the training enabled the models to exploit the in-context prompts in this setting. LLMs may possess the capacity for performing ToM reasoning and yet not express this competence without the appropriate context or prompting. When supported by chain-of-thought and step-by-step prompting, Davinci-3 and GPT-3.5-Turbo exhibited ToM accuracy that was higher than GPT-4\u2019s zero-shot ToM accuracy. These results are not consistent with the claim that these models lack ToM reasoning capability (Sap et al., 2022), and they indicate the effectiveness of a prompting approach in enhancing LLM performance. Models may fail for different reasons. For instance, in zero-shot inference, we noticed that Davinci-2 was more accurate than GPT-3.5 Turbo overall, but the models failed in different ways. When Davinci-2 answered a question incorrectly, it tended to do so with high confidence, without speculating over other possibilities. Conversely, GPT-3.5-Turbo\u2019s errors in zero-shot inference usually arose because the model\u2019s responses were more cautious: the model refrained from drawing confident conclusions and would frequently generate responses such as there is not enough information in the scenario to answer this question. This property may arise from GPT-3.5-Turbo\u2019s training method which was intentionally designed to make the model more careful. Generating inconclusive responses was also mentioned as one of the limitations of these models (OpenAI, 2022). Thus, the fact that Davinci-2 exhibited greater zero-shot ToM accuracy than GPT-3.5-Turbo is not conclusive evidence that Davinci-2 is more capable at ToM reasoning. In fact, compared to GPT-3.5-Turbo, Davinci-2 produced incorrect answers with higher confidence, made more error in linking one logical step to the next, and occasionally confabulated (a class of errors called hallucinations in the machine learning literature). Therefore, lower zero-shot performance of GPT-3.5-Turbo compared to Davinci-2 could be because GPT-3.5-Turbo has a stylistic bias that prevents it from providing definitive answers in short scenarios requiring inference. Previous studies evaluating ToM performance in LLMs have primarily relied on single word completion or multiple-choice questions to measure their abilities (Kosinski, 2023; Sap et al., 2022; Trott et al., 2022; Ullman, 2023). However, this evaluation approach may not capture the sophistication of the ToM reasoning of which LLMs are capable. ToM reasoning is a complex behavior, which, even in humans, can involve multiple steps. Therefore, when responding to this challenging task, LLMs may benefit from producing longer-form answers. There are at least two reasons why LLMs may benefit in this way: First, we may be able to more fairly evaluate the model output when it is longer. LLMs sometimes generate the \u201ccorrect\u201d reasoning and then additionally mention other possibilities which lead it to arrive at an inconclusive overall summary. In such cases, the LLM is demonstrating that it is capable of correctly performing the reasoning steps for the ToM question, even though its overall conclusion does not correspond to one of a fixed set of options. Relatedly, the model might have a certain level of information regarding the",
        "chunk_index": 8,
        "start_char": 25675,
        "end_char": 29735
      },
      {
        "id": "337e8720-5621-425c-bf4d-1c5524982472",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "There are at least two reasons why LLMs may benefit in this way: First, we may be able to more fairly evaluate the model output when it is longer. LLMs sometimes generate the \u201ccorrect\u201d reasoning and then additionally mention other possibilities which lead it to arrive at an inconclusive overall summary. In such cases, the LLM is demonstrating that it is capable of correctly performing the reasoning steps for the ToM question, even though its overall conclusion does not correspond to one of a fixed set of options. Relatedly, the model might have a certain level of information regarding the potential results of a situation, but it may not be sufficient for it to draw a correct conclusion. This can be compared to the concept that humans can still have some knowledge of an object's location despite having imperfect knowledge (Wu and Wolfe, 2018). Encouraging the model to systematically examine each piece of evidence and engage in a step- by-step reasoning process could help solidify its partial evidence and enable it to arrive at a definitive response. Second, LLMs may unlock enhanced (or new) reasoning abilities when provided with the opportunity and the cues to elaborate a systematic step-by-step response. The improved ToM performance we observe is not simply a result of providing a number of in-context examples of ToM tasks (as in (Sap et al., 2022), where performance remained poor) but seems to rely on providing examples in which there is step-by-step inferential reasoning from the evidence before arriving at a conclusion (Figure 3 and Figure 4). The LLMs may have seen some ToM or Photo scenarios during their training phase, but data leakage is unlikely to affect our findings. First, our findings concern the change in performance arising from prompting, and the specific prompts used to obtain this performance change were novel materials generated for this study. Second, if the model performance relied solely on prior exposure to the training data, there should be little difference between zero-shot Photo and ToM performance (Figure 2), as these materials were published in the same documents; however, the zero-shot performance patterns were very different across Photo and ToM scenarios. Third, the LLM performance improvements arose when the models elaborated their reasoning step-by-step, and this elaborated reasoning was not part of the training data. Therefore, although some data leakage is possible, it is unlikely to affect our conclusions concerning the benefits of prompting. An important avenue for further testing is whether the prompt-driven performance gains are specific to ToM reasoning, or would be expected more generally in tasks involving other forms of inferential reasoning. Many of the ToM questions require the model to infer facts (e.g. mental states) that are not explicitly stated in the question, while (qualitatively speaking) it seems that many of the Control scenarios can be answered without performing as much inference beyond what is explicitly provided in the scenario text. Therefore, we are now testing LLMs comprehension in scenarios that require inferential reasoning but not reasoning about people\u2019s ToM. Our preliminary results indicate (i) a similar pattern in zero-shot performance for ToM scenarios and non- ToM scenarios which require inferential reasoning; and (ii) an improvement in non-ToM performance when incorporating the same prompts used for ToM scenarios (see Supplement E). Future research is needed to further explore the inferential reasoning capacity of LLMs as well as whether ToM inferences are a representative example of a more general set of inferential capabilities in LLMs. We note four areas for improvement of this work. First, to evaluate ToM performance in our main analyses, we tested the effects of CoT prompting",
        "chunk_index": 9,
        "start_char": 29140,
        "end_char": 32960
      },
      {
        "id": "d1a9fca6-d48f-4baf-827c-f35f46a60a2a",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "ToM. Our preliminary results indicate (i) a similar pattern in zero-shot performance for ToM scenarios and non- ToM scenarios which require inferential reasoning; and (ii) an improvement in non-ToM performance when incorporating the same prompts used for ToM scenarios (see Supplement E). Future research is needed to further explore the inferential reasoning capacity of LLMs as well as whether ToM inferences are a representative example of a more general set of inferential capabilities in LLMs. We note four areas for improvement of this work. First, to evaluate ToM performance in our main analyses, we tested the effects of CoT prompting using only 2 CoT example scenarios, and we tested only 16 ToM questions, which were mostly probing agents\u2019 beliefs. Future research could explore the effects of different number of CoT examples, using various types of CoT examples, and examine a more diverse set of ToM tasks (Ullman, 2023). Second, in GPT-3.5 models, sometimes the reasoning was correct, but the model could not integrate that reasoning to draw the correct conclusion. Future research should extend the investigation of methods (such as RLHF) that can help LLMs draw a correct conclusion given the prior reasoning steps. Third, in the current study, we did not quantitatively analyze the failure modes of each model. To address the limitations of LLMs and further improve their reasoning capabilities, it is important to extend our understating of how and why different models fail. Moreover, we observed significant variability in performance across scenarios (Figure S.1). Therefore, measures of mean performance should be augmented by examination of failure modes and settings which may be specific to particular types of reasoning or subcomponents of ToM (Burnell et al., 2023). Finally, in the present study we manually scored the LLM responses. Because this form of labeling could be prone to individuals\u2019 interpretations, we are sharing the raw LLM outputs that were the basis of our findings (see Data Availability).1 We await established benchmarks for evaluating complex reasoning and ToM behaviors in LLMs. Our data do not speak to the question of whether LLMs possess a \"mental faculty\" that corresponds to a structured logical model of mental states. But our data do suggest that, when asking questions about ToM in LLMs, it will not be fruitful to seek a simple yes/no answer. The variation in performance across prompts may be analogous to how human cognition can vary across task contexts and motivational states, and how humans draw on more than one type of thinking (Evans, 2003). In LLMs, it is clear that task contexts (i.e. prompts) affect not only bottom-line accuracy, but, more qualitatively, the model\u2019s ability to invoke appropriate modes and styles of responding. Our results are practically significant because they show how to aid LLMs in some forms of social reasoning. More abstractly, our results are another reminder that LLM behavior is highly complex and context sensitive. Therefore, it will be important to characterize their cognitive abilities via nuanced investigations (Firestone, 2020; Mitchell and Krakauer, 2022; Sejnowski, 2023), rather than reflexively applying existing cognitive ontologies. Also, as we build and interact with increasingly powerful cognitive artifacts, it is crucial to stretch our imaginations about what they are capable of and how they work. Conclusion We have shown that LLMs can exploit chain-of-thought reasoning and step-by-step thinking to substantially improve their ToM performance. Human-level performance in these ToM scenarios was 87% (\u00b1 4%). In contrast to zero-shot ToM settings, where only GPT-4 reached near 80% accuracy, with appropriate prompting, all RLHF-trained models exceeded 80% accuracy, with GPT-4 reaching ceiling accuracy (100%). Thus, appropriate prompting enhances the ToM reasoning performance of these",
        "chunk_index": 10,
        "start_char": 32317,
        "end_char": 36228
      },
      {
        "id": "7c36c656-1204-4c93-a6df-c82f7f0d7d30",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "reflexively applying existing cognitive ontologies. Also, as we build and interact with increasingly powerful cognitive artifacts, it is crucial to stretch our imaginations about what they are capable of and how they work. Conclusion We have shown that LLMs can exploit chain-of-thought reasoning and step-by-step thinking to substantially improve their ToM performance. Human-level performance in these ToM scenarios was 87% (\u00b1 4%). In contrast to zero-shot ToM settings, where only GPT-4 reached near 80% accuracy, with appropriate prompting, all RLHF-trained models exceeded 80% accuracy, with GPT-4 reaching ceiling accuracy (100%). Thus, appropriate prompting enhances the ToM reasoning performance of these highly context-sensitive models. 1 Note that only a small portion of the data are edge cases which require subjective evaluation. A sample of edge case can be seen in Example 6 in the Correct Responses section. Furthermore, Davinci-2, Davinci-3, and GPT-4 were generally able to deliver decisive outcomes. Most of these edge cases primarily originated from GPT-3.5-Turbo, which was apparently finetuned to exercise caution in its responses. Acknowledgments The authors gratefully acknowledge the support of the National Institutes of Mental Health (Grant R01MH119099). We further thank members of the Firestone Lab at Johns Hopkins University for helpful feedback on earlier drafts of this paper. Data Availability The data used in this study are available at the following GitHub repository: https://github.com/shrahimim/Boosting-Theory-of-Mind-in-LLMs-with-Prompting References Bedny, M., Pascual-Leone, A., and Saxe, R. R. (2009). Growing up blind does not change the neural bases of Theory of Mind. Proceedings of the National Academy of Sciences of the United States of America, 106(27), 11312\u201311317. https://doi.org/10.1073/pnas.0900010106 Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., \u2026 Amodei, D. (2020). Language models are few-shot learners. Advances in Neural Information Processing Systems, 2020-Decem. Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., Nori, H., Palangi, H., Ribeiro, M. T., and Zhang, Y. (2023). Sparks of Artificial General Intelligence: Experiments with an early version of GPT-4. Burnell, R., Schellaert, W., Burden, J., Ullman, T. D., Martinez-Plumed, F., Tenenbaum, J. B., Rutar, D., Cheke, L. G., Sohl-Dickstein, J., Mitchell, M., Kiela, D., Shanahan, M., Vorrhees, E. M., Cohn, A. G., Leibo, J. Z., and Hernandez-Orallo, J. (2023). Rethink reporting of evaluation results in AI. 380(6641), 8\u201311. https://doi.org/10.1126/science.adf6369 Cuzzolin, F., Morelli, A., C\u00eerstea, B., and Sahakian, B. J. (2020). Knowing me, knowing you: Theory of mind in AI. Psychological Medicine, 50(7), 1057\u20131061. https://doi.org/10.1017/S0033291720000835 Dodell-Feder, D., Koster-Hale, J., Bedny, M., and Saxe, R. (2011). FMRI item analysis in a theory of mind task. NeuroImage, 55(2), 705\u2013712. https://doi.org/10.1016/j.neuroimage.2010.12.040 Evans, J. S. B. T. (2003). In two minds: Dual-process accounts of reasoning. Trends in Cognitive Sciences, 7(10), 454\u2013459. https://doi.org/10.1016/j.tics.2003.08.012 Firestone, C. (2020). Performance vs. competence in human\u2013machine comparisons. Proceedings of the National Academy of Sciences of the United States of America, 117(43), 26562\u201326571. https://doi.org/10.1073/pnas.1905334117 Hahn, M., and Goyal, N. (2023). A Theory of Emergent In-Context Learning as Implicit Structure Induction. http://arxiv.org/abs/2303.07971 Heyes, C. M., and Frith, C. D. (2014). The cultural evolution of mind reading. Science, 344(6190). https://doi.org/10.1126/science.1243091 Kidd, D. C., and Castano, E. (2013). Reading literary fiction improves theory of mind. Science, 342(6156), 377\u2013380. https://doi.org/10.1126/science.1239918 Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. (2022). Large Language Models are Zero-Shot Reasoners. NeurIPS. http://arxiv.org/abs/2205.11916 Kosinski, M. (2023). Theory of Mind May Have Spontaneously Emerged in Large Language Models. ArXiv. https://doi.org/https://doi.org/10.48550/arXiv.2302.02083 Krupenye, C., and Call, J. (2019). Theory of mind",
        "chunk_index": 11,
        "start_char": 35516,
        "end_char": 39944
      },
      {
        "id": "faed4653-1b8c-429e-be56-bea7aea0451c",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "26562\u201326571. https://doi.org/10.1073/pnas.1905334117 Hahn, M., and Goyal, N. (2023). A Theory of Emergent In-Context Learning as Implicit Structure Induction. http://arxiv.org/abs/2303.07971 Heyes, C. M., and Frith, C. D. (2014). The cultural evolution of mind reading. Science, 344(6190). https://doi.org/10.1126/science.1243091 Kidd, D. C., and Castano, E. (2013). Reading literary fiction improves theory of mind. Science, 342(6156), 377\u2013380. https://doi.org/10.1126/science.1239918 Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. (2022). Large Language Models are Zero-Shot Reasoners. NeurIPS. http://arxiv.org/abs/2205.11916 Kosinski, M. (2023). Theory of Mind May Have Spontaneously Emerged in Large Language Models. ArXiv. https://doi.org/https://doi.org/10.48550/arXiv.2302.02083 Krupenye, C., and Call, J. (2019). Theory of mind in animals: Current and future directions. Wiley Interdisciplinary Reviews: Cognitive Science, 10(6), 1\u201325. https://doi.org/10.1002/wcs.1503 Langley, C., Cirstea, B. I., Cuzzolin, F., and Sahakian, B. J. (2022). Theory of Mind and Preference Learning at the Interface of Cognitive Science, Neuroscience, and AI: A Review. Frontiers in Artificial Intelligence, 5(April), 1\u201317. https://doi.org/10.3389/frai.2022.778852 Magister, L. C., Mallinson, J., Adamek, J., Malmi, E., and Severyn, A. (2022). Teaching Small Language Models to Reason. http://arxiv.org/abs/2212.08410 Mahowald, K., Ivanova, A. A., Blank, I. A., Kanwisher, N., Tenenbaum, J. B., and Fedorenko, E. (2023). Dissociating language and thought in large language models: a cognitive perspective. 2. http://arxiv.org/abs/2301.06627 Mitchell, M., and Krakauer, D. C. (2022). The Debate Over Understanding in AI\u2019s Large Language Models. 1\u201313. http://arxiv.org/abs/2210.13966 Moran, J. M., Young, L. L., Saxe, R., Lee, S. M., O\u2019Young, D., Mavros, P. L., and Gabrieli, J. D. (2011). Impaired theory of mind for moral judgment in high-functioning autism. Proceedings of the National Academy of Sciences of the United States of America, 108(7), 2688\u20132692. https://doi.org/10.1073/pnas.1011734108 OpenAI. (2022). Introducing ChatGPT. https://openai.com/blog/chatgpt OpenAI. (2023a). GPT-4 Technical Report. 4, 1\u2013100. http://arxiv.org/abs/2303.08774 OpenAI. (2023b). GPT Models Documentation. https://platform.openai.com/docs/models/overview OpenAI. (2023c). Model index for researchers. https://platform.openai.com/docs/model-index-for-researchers Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. (2022). Training language models to follow instructions with human feedback. http://arxiv.org/abs/2203.02155 Povinelli, D. J., and Preuss, T. M. (1995). Theory of mind: evolutionary history of a cognitive specialization. Trends in Neurosciences, 18(9), 418\u2013424. https://doi.org/10.1016/0166-2236(95)93939-U Prystawski, B., and Goodman, N. D. (2023). Why think step-by-step? Reasoning emerges from the locality of experience. http://arxiv.org/abs/2304.03843 Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A., Powell, R., Driessche, G. van den, Hendricks, L. A., Rauh, M., Huang, P.-S., \u2026 Irving, G. (2021). Scaling Language Models: Methods, Analysis & Insights from Training Gopher. http://arxiv.org/abs/2112.11446 Sap, M., Le Bras, R., Fried, D., and Choi, Y. (2022). Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, 3762\u20133780. Sejnowski, T. J. (2023). Large Language Models and the Reverse Turing Test. Neural Computation, 35(3), 309\u2013 342. https://doi.org/10.1162/neco_a_01563 Seyfarth, R. M., and Cheney, D. L. (2013). Affiliation, empathy, and the origins of Theory of Mind. Proceedings of the National Academy of Sciences of the United States of America, 110(SUPPL2), 10349\u201310356. https://doi.org/10.1073/pnas.1301223110 Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. (2020). Learning to summarize from human feedback. Advances in Neural Information Processing Systems, 2020-Decem(NeurIPS), 1\u201314. Trott, S., Jones, C., Chang, T., Michaelov, J., and Bergen, B. (2022). Do Large Language Models know what humans know? http://arxiv.org/abs/2209.01515 Ullman, T. (2023). Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks. http://arxiv.org/abs/2302.08399",
        "chunk_index": 12,
        "start_char": 39094,
        "end_char": 43793
      },
      {
        "id": "1fca5ec6-ce2c-4552-9791-2be1dd6d8a7d",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "and Cheney, D. L. (2013). Affiliation, empathy, and the origins of Theory of Mind. Proceedings of the National Academy of Sciences of the United States of America, 110(SUPPL2), 10349\u201310356. https://doi.org/10.1073/pnas.1301223110 Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. (2020). Learning to summarize from human feedback. Advances in Neural Information Processing Systems, 2020-Decem(NeurIPS), 1\u201314. Trott, S., Jones, C., Chang, T., Michaelov, J., and Bergen, B. (2022). Do Large Language Models know what humans know? http://arxiv.org/abs/2209.01515 Ullman, T. (2023). Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks. http://arxiv.org/abs/2302.08399 Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. (2022). Chain-of- Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS, 1\u201343. http://arxiv.org/abs/2201.11903 Wu, C. C., and Wolfe, J. M. (2018). A New Multiple Object Awareness Paradigm Shows that Imperfect Knowledge of Object Location Is Still Knowledge. Current Biology, 28(21), 3430-3434.e3. https://doi.org/10.1016/j.cub.2018.08.042 Young, L., Cushman, F., Hauser, M., and Saxe, R. (2007). The neural basis of the interaction between theory of mind and moral judgment. Proceedings of the National Academy of Sciences of the United States of America, 104(20), 8235\u20138240. https://doi.org/10.1073/pnas.0701408104 Supplementary Material: Supplement A 16 Photo scenarios These scenarios are adapted and modified from (Dodell-Feder et al., 2011): 1. \"The traffic camera snapped an image of the black car as it sped through the stoplight. Soon after, the car was painted red and the license plates were changed.\" Q: Does the traffic camera show that the car is black? 2. \"A map shows the ground floor plan. A photocopy was sent to the architect yesterday, but at the time the kitchen door was missing. It was added to the map this morning.\" Q: Does the architect\u2019s photocopy show the door? 3. \"A photograph was taken of an apple hanging on a tree branch. The film took half an hour to develop. In the meantime, a strong wind blew the apple to the ground.\" Q: Does the developed photograph show the apple on the tree? 4. \"To detect intruders, the lab uses an automated system for recording voices. In the empty lab one night, a computer error occurs and a synthetic voice reads the error message.\" Q: Is the number of people in the lab that night zero? 5. \"A popular attraction in the park, pictured on many souvenirs, was a cliff face covered with ancient petroglyphs. Recently, the petroglyphs crumbled and scientists have not begun to restore them.\" Q: Today, can the petroglyphs be seen in the park? 6. \"Sargent famously painted the south bank of the river in 1885. In 1910 a huge dam was built, flooding out the whole river basin, killing the old forests. Now the whole area is under water.\" Q: In the painting, is the south bank of the river wooded? 7. \"When the picture was taken of the house, it was one story tall. Since then, the renovators added an additional story and a garage.\" Q: Is the house currently one story? 8. \"Accounts of the country's economic success were recorded in books from the early 1900s. Soon after, a horrible plague hit the country, and the country was sent into an economic depression.\" Q: Do early 1900s novels portray the country as experiencing economic wealth? 9. \"Part of the garden is supposed to be reserved for the roses; it's labeled accordingly. Recently the garden has run wild, and dandelions have taken over the entire flower bed.\" Q: Does the label say these flowers are roses? 10. \"A long time ago, an explorer mapped",
        "chunk_index": 13,
        "start_char": 43044,
        "end_char": 46773
      },
      {
        "id": "c89b41bd-e0a0-4d02-9175-4ce33cb63c91",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "garage.\" Q: Is the house currently one story? 8. \"Accounts of the country's economic success were recorded in books from the early 1900s. Soon after, a horrible plague hit the country, and the country was sent into an economic depression.\" Q: Do early 1900s novels portray the country as experiencing economic wealth? 9. \"Part of the garden is supposed to be reserved for the roses; it's labeled accordingly. Recently the garden has run wild, and dandelions have taken over the entire flower bed.\" Q: Does the label say these flowers are roses? 10. \"A long time ago, an explorer mapped a small island. Since then, the water levels rose and only a tiny part of the island is now left above water.\" Q: On the explorer's maps, does the island appear to be mostly above water? 11. \"A large oak tree stood in front of City Hall from the time the building was built. Last year the tree fell down and was replaced by a stone fountain.\" Q: Does an antique drawing of City Hall show a fountain in front? 12. \"A volcano erupted on a Caribbean island three months ago. Barren lava rock is all that remains today. Satellite photographs show the island as it was before the eruption.\" Q: Do satellite photographs show the island is covered in lava? 13. \"The family's old video tape recorded the daughter's first birthday party at their house in Chicago. Since then, the family sold their house and moved to San Francisco.\" Q: Does the video show the family living in San Francisco? 14. \"At the time a portrait was drawn of a young man, he had short brown hair and no facial hair. Now the man's hair is long and gray and so is his beard.\" Q: Today, is the length of the man\u2019s beard long? 15. \"The girl's middle school pictures showed her wearing a white blouse. Later, a red sock was accidentally washed with the blouse and the blouse turned pink.\" Q: Was the color of the blouse in the pictures pink? 16. \"A small leaf was placed on a wet clay flower pot. When the pot was baked at high temperatures to harden the clay, the leaf crumbled, but its impression remained.\" Q: Is the actual leaf intact? Supplement B 16 ToM scenarios & the scenarios used for in-context learning: 16 ToM scenarios are adapted and modified from (Dodell-Feder et al., 2011). 1. \"The weather was so warm today that all the tulips in Pam's backyard suddenly bloomed. The tulips next to Pam's office still have not yet flowered, though. Pam has been at work all day.\" Q: When Pam is driving home after work, does she assume her tulips have bloomed? A: 2. \"Susie parked her sports car in the driveway. In the middle of the night, Nathan moved her car into the garage to make room for his minivan. Susie woke up early in the morning.\" Q: When Susie wakes up, does she see the minivan in the driveway? A: 3. \"The morning of the high school dance Sarah placed her high heel shoes under her dress and then went shopping. That afternoon, her sister borrowed the shoes and later put them under Sarah's bed.\" Q: When Sarah gets ready, does she assume her shoes are under her dress? A: 4. \"Sally and Greg called ahead of time to make a reservation for the back-country cabin. The park ranger forgot to write down the reservation and two other hikers got to the cabin first.\" Q: When Sally and Greg arrive, do they find their cabin unoccupied?",
        "chunk_index": 14,
        "start_char": 46188,
        "end_char": 49481
      },
      {
        "id": "2599d77a-4f65-4a02-8a47-90c7f893aba1",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "in the driveway? A: 3. \"The morning of the high school dance Sarah placed her high heel shoes under her dress and then went shopping. That afternoon, her sister borrowed the shoes and later put them under Sarah's bed.\" Q: When Sarah gets ready, does she assume her shoes are under her dress? A: 4. \"Sally and Greg called ahead of time to make a reservation for the back-country cabin. The park ranger forgot to write down the reservation and two other hikers got to the cabin first.\" Q: When Sally and Greg arrive, do they find their cabin unoccupied? A: 5. \"When Jeff got ready this morning, he put on a light pink shirt instead of a white one. Jeff is colorblind, so he can't tell the difference between subtle shades of color.\" Q: Does Jeff believe he is wearing a white shirt? A: 6. \"Larry chose a debated topic for his class paper due on Friday. The news on Thursday indicated that the debate had been solved, but Larry never read it.\" Q: When Larry writes his paper, does he think the debate has been solved? A: 7. \"Every day Jill goes to the coffee shop on the corner and orders a latte, her favorite drink. Today, the cashier misunderstands Jill and prepares a mocha instead.\" Q: Does Jill think her drink will taste like a mocha? A: 8. \"Expecting the game to be postponed because of the rain, the Garcia family took the subway home. The score was tied, 3-3. During their commute the rain stopped, and the game soon ended with a score of 5- 3.\" Q: When the Garcia family arrives home, do they believe the score is 5-3? A: 9. \"Amy walked to work today. When George woke up, he saw her car in the drive. Her room was quiet and dark. George knows that when Amy is sick, she lies down in a dark room.\" Q: Does George think Amy is sick today? A: 10. \"John told Mary that he had lost his keys. The two of them searched the house with no luck. Then Mary went outside to look in the car. Suddenly John noticed his keys behind the sofa.\" Q: By the time Mary comes in, does John know where his keys are? A: 11. \"At night a bear broke into a cooler near a tent and drank the soda. Five hours later, the campers woke up and went to their cooler for breakfast.\" Q: Do the campers find their cooler empty of soda? A: 12. \"When Lisa left Jacob, he was deep asleep on the beach. A few minutes later a wave woke him. Seeing Lisa was gone, Jacob decided to go swimming.\" Q: Does Lisa now believe that Jacob is asleep? A: 13. \"Jenny put her chocolate away in the cupboard. Then she went outside. Alan moved the chocolate from the cupboard into the fridge. Half an hour later, Jenny came back inside.\" Q: Does Jenny expect to find her chocolate in the cupboard? A: 14. \"A window wiper was commissioned by a CEO to wipe an entire building. He finished the right side, but his platform broke before he could do the left side. The next morning the CEO arrived with foreign investors.\" Q: When the CEO comes to work, do they discover that all of the windows are cleaned? A: 15. \"Hopeful to catch a prize fish, George went fishing. That afternoon, he saw his fishing line bend over as if he had caught a big fish. Actually, George\u2019s fishing pole had snagged a small tire.\" Q:",
        "chunk_index": 15,
        "start_char": 48930,
        "end_char": 52088
      },
      {
        "id": "557b69eb-e109-4694-9042-5e05202dba47",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "her chocolate in the cupboard? A: 14. \"A window wiper was commissioned by a CEO to wipe an entire building. He finished the right side, but his platform broke before he could do the left side. The next morning the CEO arrived with foreign investors.\" Q: When the CEO comes to work, do they discover that all of the windows are cleaned? A: 15. \"Hopeful to catch a prize fish, George went fishing. That afternoon, he saw his fishing line bend over as if he had caught a big fish. Actually, George\u2019s fishing pole had snagged a small tire.\" Q: At the end of the fishing line, does George see a fish? A: 16. \"Laura didn't have time to braid her horse\u2019s mane before going to camp. While she was at camp, William brushed Laura's horse and braided the horse\u2019s mane for her.\" Q: When Laura is at camp, does she assume that her horse's hair is braided? A: The scenarios used in the chain-of-thought reasoning examples: Example 1: \"Anne made lasagna in the blue dish. After Anne left, Ian came home and ate the lasagna. Then he filled the blue dish with spaghetti and replaced it in the fridge.\" Q: Does Anne think the blue dish contains spaghetti? A: When Anne left, the blue dish contained lasagna. Ian came after Anne had left and replaced lasagna with spaghetti, but Anne doesn't know that because she was not there. So, the answer is: No, she doesn't think the blue dish contains spaghetti. Example 2: \"The girls left ice cream in the freezer before they went to sleep. Over night the power to the kitchen was cut and the ice cream melted.\" Q: When they get up, do the girls believe the ice cream is melted? A: The girls put the ice cream in the freezer and went to sleep. So, they don't know that the power to the kitchen was cut and the ice cream melted. So, the answer is: No, the girls don't believe the ice cream is melted. Supplement C ToM accuracy of human participants To measure humans\u2019 performance in ToM and Photo scenarios, we recruited 125 online participants through the Qualtrics platform. Participants were 18 to 65 years old, native English speakers, and located in the United States. Participants had 18 seconds to read each scenario. Once the 18-second duration was over, they were immediately directed to a new screen showing the comprehension question. They could respond to the questions at their own pace by clicking on one of the two options (Yes/No). The Photo scenarios and ToM scenarios were interleaved and randomized such that each participant experienced both types of scenarios. The Photo and ToM scenarios were of comparable difficulty for human participants. Participants showed similar accuracy across Photo (86% \u00b14%) and ToM (87% \u00b14%) scenarios. From these data we conclude that: i) the Photo scenarios are a good control condition for the ToM scenarios, because of their matched performance in humans. ii) the scenarios are relatively easy for humans to solve, but not so easy that humans are at ceiling performance (given the modest incentives for accuracy in online participants). Making a direct quantitative comparison between human and LLM performance is not warranted, because of the many differences in the testing conditions. However, the human performance does indicate that the questions are sufficiently difficult that humans occasionally (>10% of the time) make errors, perhaps because they overlook details in the scenario or because they make unusual assumptions or inferences. Supplement D Effects of Various In-context Examples on ToM Performance of LLMs We tested the efficacy",
        "chunk_index": 16,
        "start_char": 51549,
        "end_char": 55072
      },
      {
        "id": "0ca0f168-d790-40ab-bc3c-c6a9d54b5c15",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "relatively easy for humans to solve, but not so easy that humans are at ceiling performance (given the modest incentives for accuracy in online participants). Making a direct quantitative comparison between human and LLM performance is not warranted, because of the many differences in the testing conditions. However, the human performance does indicate that the questions are sufficiently difficult that humans occasionally (>10% of the time) make errors, perhaps because they overlook details in the scenario or because they make unusual assumptions or inferences. Supplement D Effects of Various In-context Examples on ToM Performance of LLMs We tested the efficacy of Photo examples, Non-ToM Inferential examples, and ToM examples in enhancing ToM performance when providing chain-of-thought prompts. We used the following examples in each class: 1) Photo Chain-of-Thought Examples: Example 1: Scenario: \"A long time ago, an explorer mapped a small island. Since then, the water levels rose and only a tiny part of the island is now left above water.\" Q: On the explorer's maps, does the island appear to be mostly above water? A: An explorer mapped an island a long time ago. Then water levels rose. Now only a tiny part of the island is above water. The maps show the island before water levels rose. Therefore, in the map most of the island is above water. So, the answer is: Yes, in the maps, most of the island is above the water. Example 2: Scenario: \"A volcano erupted on a Caribbean island three months ago. Barren lava rock is all that remains today. Satellite photographs show the island as it was before the eruption.\" Q: Do satellite photographs show the island is covered in lava? A: A volcano erupted three months ago and as a result lava rock is all that remains today. Since the photograph shows the island before the eruptions, it does not show the island covered in Laval. So, the answer is: No, the photographs does not show the island covered in lava. 2) Non-ToM Inferential Chain-of-Thought Examples: Example 1: Scenario: \"Sarah was excited to show off her new dress at the party. As she walked in, she noticed that another girl was wearing the exact same dress. She quickly went back home to change and then returned to the party.\" Q: Did Sarah wear her favorite dress the whole night? A: Sarah was excited about her new dress and wanted to wear it as the party. But since another girl was wearing the same dress, she went home and changed to a different dress. So, the answer is: No, she did not wear her favorite dress the whole night. Example 2: Scenario: \"Lucy was in a hurry to get to work, so she quickly grabbed her lunch from the fridge and left the house. At lunchtime, she opened her lunchbox and realized she had accidentally taken her roommate's lunch.\" Q: Will Lucy's roommate have her own lunch to eat that day? A: Lucy was in hurry and mistakenly grabbed her roommate lunch. So, her roommate\u2019s lunch is now with Lucy. So, the answer is: No, Lucy's roommate will not have her own lunch to eat that day. 3) ToM Chain-of-Thought Examples: Example 1: Scenario: \"Anne made lasagna in the blue dish. After Anne left, Ian came home and ate the lasagna. Then he filled the blue dish with spaghetti and replaced it in the fridge.\" Q: Does Anne think the blue dish contains spaghetti? A: When Anne left the blue dish contained lasagna. Ian came after Anne had left and replaced lasagna with spaghetti, but Anne doesn't know that because she was",
        "chunk_index": 17,
        "start_char": 54403,
        "end_char": 57879
      },
      {
        "id": "9dd896eb-257d-41fb-b4a4-974b2660be8f",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "So, her roommate\u2019s lunch is now with Lucy. So, the answer is: No, Lucy's roommate will not have her own lunch to eat that day. 3) ToM Chain-of-Thought Examples: Example 1: Scenario: \"Anne made lasagna in the blue dish. After Anne left, Ian came home and ate the lasagna. Then he filled the blue dish with spaghetti and replaced it in the fridge.\" Q: Does Anne think the blue dish contains spaghetti? A: When Anne left the blue dish contained lasagna. Ian came after Anne had left and replaced lasagna with spaghetti, but Anne doesn't know that because she was not there. So, the answer is: No, she doesn't think the blue dish contains spaghetti. Example 2: Scenario: \"The girls left ice cream in the freezer before they went to sleep. Over night the power to the kitchen was cut and the ice cream melted.\" Q: When they get up, do the girls believe the ice cream is melted? A: The girls put the ice cream in the freezer and went to sleep. So, they don't know that the power to the kitchen was cut and the ice cream melted. So, the answer is: No, the girls don't believe the ice cream is melted. GPT-4 For this analysis, the performance of GPT-4 was assessed on the 4 scenarios below. These are scenarios that GPT-4 answered incorrectly in zero-shot testing. Scenario 1: \"Amy walked to work today. When George woke up, he saw her car in the drive. Her room was quiet and dark. George knows that when Amy is sick, she lies down in a dark room.\" Q: Does George think Amy is sick today? Scenario 2: \"When Lisa left Jacob, he was deep asleep on the beach. A few minutes later a wave woke him. Seeing Lisa was gone, Jacob decided to go swimming.\" Q: Does Lisa now believe that Jacob is asleep? Scenario 3: \"The weather was so warm today that all the tulips in Pam's backyard suddenly bloomed. The tulips next to Pam's office still have not yet flowered, though. Pam has been at work all day.\" Q: When Pam is driving home after work, does she assume her tulips have bloomed? Scenario 4: \"Every day Jill goes to the coffee shop on the corner and orders a latte, her favorite drink. Today, the cashier misunderstands Jill and prepares a mocha instead.\" Q: Does Jill think her drink will taste like a mocha? Davinci-3 For this analysis, the performance of Davinci-3 was assessed on the 6 scenarios below. These are scenarios that Davinci-3 answered incorrectly in zero-shot testing. Scenario 1) \"Susie parked her sports car in the driveway. In the middle of the night, Nathan moved her car into the garage to make room for his minivan. Susie woke up early in the morning.\" Q: When Susie wakes up, does she see the minivan in the driveway? Scenario 2) \"When Jeff got ready this morning, he put on a light pink shirt instead of a white one. Jeff is colorblind, so he can't tell the difference between subtle shades of color.\" Q: Does Jeff believe he is wearing a white shirt? Scenario 3) \"When Lisa left Jacob, he was deep asleep on the beach. A few minutes later a wave woke him. Seeing Lisa was gone, Jacob decided to go swimming.\" Q: Does Lisa now believe that Jacob is asleep? Scenario 4) \"Laura didn't have time to braid her horse\u2019s mane before going to camp. While she was at camp, William brushed Laura's horse and braided the horse\u2019s mane for her.\" Q: When Laura is at camp,",
        "chunk_index": 18,
        "start_char": 57320,
        "end_char": 60589
      },
      {
        "id": "34a3ccdd-afbe-46e9-b4a2-38c719fbe595",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "Jeff is colorblind, so he can't tell the difference between subtle shades of color.\" Q: Does Jeff believe he is wearing a white shirt? Scenario 3) \"When Lisa left Jacob, he was deep asleep on the beach. A few minutes later a wave woke him. Seeing Lisa was gone, Jacob decided to go swimming.\" Q: Does Lisa now believe that Jacob is asleep? Scenario 4) \"Laura didn't have time to braid her horse\u2019s mane before going to camp. While she was at camp, William brushed Laura's horse and braided the horse\u2019s mane for her.\" Q: When Laura is at camp, does she assume that her horse's hair is braided? Scenario 5) \"The morning of the high school dance Sarah placed her high heel shoes under her dress and then went shopping. That afternoon, her sister borrowed the shoes and later put them under Sarah's bed.\" Q: When Sarah gets ready, does she assume her shoes are under her dress? Scenario 6) \"Jenny put her chocolate away in the cupboard. Then she went outside. Alan moved the chocolate from the cupboard into the fridge. Half an hour later, Jenny came back inside.\" Q: Does Jenny expect to find her chocolate in the cupboard? Per-scenario accuracies are shown in Figure S.1. The mean accuracy for this analysis is shown in Figure 4. Figure S.1. Per-Scenario Accuracy using 4 prompting conditions in GPT-4 and Davinci-3. For each condition (e.g. Inferential CoT), the per-scenario accuracy is calculated based on the mean accuracy across 20 repetitions of a single prompt. Supplement E Generalizability of ToM Results to Inferential Reasoning in Non-ToM Scenarios To investigate the generalizability of our prompting methods, we designed and tested a set of scenarios that required inferential reasoning, but which were not false-belief scenarios. In these new scenarios, the reasoning does not follow the same steps as in the false-belief scenarios (e.g. person P does not know about event E, etc). Therefore, the specific reasoning steps used in the in-context ToM CoT examples cannot be directly transferred to these non-ToM scenarios. We created and tested the following non-ToM scenarios: 1. \"Anna was baking cookies for her friends. She realized she was out of sugar and went to the store to buy some. While she was gone, her dog, Max, jumped on the counter and ate the cookie dough.\" Q: Will Anna be able to finish making cookies after returning? 2. \"Lisa was running late for work, so she quickly grabbed her lunch out of the fridge and left the house. Seeing Lisa leaving the house, her cat knocked over her coffee mug on the kitchen counter.\" Q: Will Lisa find the kitchen counter clean when she returns? 3. \"Lucy was in a hurry to get to work, so she quickly grabbed her lunch from the fridge and left the house. At lunchtime, she opened her lunchbox and realized she had accidentally taken her roommate's lunch.\" Q: Will Lucy's roommate have her own lunch to eat that day? 4. \"Lucy was baking a cake for her friend's birthday. She put the cake in the oven and set a 20-minute timer. She then went to the living room to watch her favorite show for 30 minutes.\" Q: Will the cake be overcooked by the time Lucy checks the oven? 5. \"Tim and Amy went on a hike through the forest. After walking for several hours, they came across a bear. They quickly and quietly backed away, retracing their steps back to the starting point.\" Q: Did Tim and Amy finish their hike? 6. \"Sarah was excited to show off her new dress at",
        "chunk_index": 19,
        "start_char": 60048,
        "end_char": 63464
      },
      {
        "id": "ad023276-774e-4446-a5bf-8e4d263b5976",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "a cake for her friend's birthday. She put the cake in the oven and set a 20-minute timer. She then went to the living room to watch her favorite show for 30 minutes.\" Q: Will the cake be overcooked by the time Lucy checks the oven? 5. \"Tim and Amy went on a hike through the forest. After walking for several hours, they came across a bear. They quickly and quietly backed away, retracing their steps back to the starting point.\" Q: Did Tim and Amy finish their hike? 6. \"Sarah was excited to show off her new dress at the party. As she walked in, she noticed that another girl was wearing the exact same dress. She quickly went back home to change and then returned to the party.\" Q: Did Sarah wear her favorite dress the whole night? 7. \"During a thunderstorm, Sarah closed all the windows in her house. She then went to the basement to do laundry. While she was downstairs, a tree branch fell and broke one of the windows upstairs.\" Q: When Sarah finishes the laundry, does she find all the windows closed? 8. \"Lucy spent hours preparing a delicious meal for her husband's surprise birthday party. Just as she was setting the table, she accidentally knocked over a glass of red wine, spilling it all over the food. She ordered pizza as a last-minute replacement.\" Q: Does Lucy serve her homemade meal to the guests? We tested two conditions: (1) The zero-shot performance of the LLMs in these new scenarios; (2) The efficacy of the same prompts, Two-shot chain-of-thought ToM examples plus step-by-step thinking, for these new set of non-ToM scenarios. We found a similar pattern between the performance in the non-ToM scenarios and the ToM scenarios (Figure S.2): First, similar to results from Figure 2B, the zero-shot accuracy dropped from Davinci-2 to Davinci-3, and from Davinci-3 to GPT-3.5-Turbo2, while GPT-4 showed the highest accuracy, reaching ceiling performance. Second, similar to the results from Figure 3, we found that prompting significantly boosted the ToM accuracy for Davinci-3 and GPT-3.5-Turbo, while it did not improve the accuracy of Davinci-2. The performance of GPT- 4 was at ceiling in the zero-shot setting, so there was no room for benefit from prompting. Thus, this preliminary analysis indicates that the performance gains from prompting are not limited to inferential ToM tasks, but may extend to other inferential tasks. This result is notable for two reasons: (i) The reasoning steps for the chain-of-thought examples (which were ToM examples) were different from the reasoning steps required to answer the questions (which were non-ToM examples). Therefore, these performance increases are not merely due to copying the specific reasoning steps in demonstrated in the prompt. (ii) These questions do not ask about an agent\u2019s state of the mind, but instead require inference about a particular situation. Therefore, it is possible that our primary findings concerning ToM may generalize to a broader class of inferential reasoning tasks. Figure S.2. Accuracy of LLMs in Non-ToM Scenarios. Accuracy of Davinci-2, Davinci-3, GPT-3.5- Turbo, and GPT-4 in new scenarios that do not involve reasoning about agents\u2019 beliefs. The gray bars (left bars) show the zero-shot performance. The orange bars (right bars) show the performance with prompts that allow for in-context learning (ICL). The ICL prompt includes both Two-shot CoT plus instructing step-by step thinking. The accuracy is the mean of 160 values (8 scenarios x 20 repetitions each). The error bars show the standard deviation of the accuracy over 8 scenarios for each of 20 repetitions (standard deviation across 20 values, each a mean of",
        "chunk_index": 20,
        "start_char": 62946,
        "end_char": 66579
      },
      {
        "id": "abbf1d9f-1f50-40d8-8751-ac7dd4ff303d",
        "document_id": "0127506d-5f2d-45c3-8eb1-8b00ccb885a0",
        "content": "in Non-ToM Scenarios. Accuracy of Davinci-2, Davinci-3, GPT-3.5- Turbo, and GPT-4 in new scenarios that do not involve reasoning about agents\u2019 beliefs. The gray bars (left bars) show the zero-shot performance. The orange bars (right bars) show the performance with prompts that allow for in-context learning (ICL). The ICL prompt includes both Two-shot CoT plus instructing step-by step thinking. The accuracy is the mean of 160 values (8 scenarios x 20 repetitions each). The error bars show the standard deviation of the accuracy over 8 scenarios for each of 20 repetitions (standard deviation across 20 values, each a mean of 8 scenarios). 2 Data from GPT-3.5-Turbo for this particular analysis was collected before it was possible to control the temperature parameter at inference time using the OpenAI API. Therefore, for this particular analysis, the temperature parameter for GPT-3.5-Turbo was the default value (likely 0.7), whereas for the other three models the temperature was set to 0.4.",
        "chunk_index": 21,
        "start_char": 65951,
        "end_char": 66950
      }
    ],
    "created_at": "2025-08-06 09:16:17.607778",
    "file_size": 1090078,
    "source_type": "pdf",
    "original_url": null
  },
  "6a29c5dd-3107-405f-bb1b-b1cee8575152": {
    "id": "6a29c5dd-3107-405f-bb1b-b1cee8575152",
    "name": "Our framework for developing safe and trustworthy agents \\ Anthropic",
    "file_path": "/Users/will/AVI BUILD/Able3_Main_WithVoiceMode/data/sources/web_content_Our_framework_for_developing_safe_and_trustworthy_agents_Anthropic.txt",
    "summary": "Policy Our framework for developing safe and trustworthy agents Aug 4, 2025 \u25cf 7 min read The most popular AI tools today are assistants that respond to specific questions or prompts. But we\u2019re now seeing the emergence of AI agents , which pursue tasks autonomously when given a goal. Think of an a...",
    "chunks": [
      {
        "id": "bcd86340-5f93-47c0-a77d-0317541b2ada",
        "document_id": "6a29c5dd-3107-405f-bb1b-b1cee8575152",
        "content": "Policy Our framework for developing safe and trustworthy agents Aug 4, 2025 \u25cf 7 min read The most popular AI tools today are assistants that respond to specific questions or prompts. But we\u2019re now seeing the emergence of AI agents , which pursue tasks autonomously when given a goal. Think of an agent like a virtual collaborator that can independently handle complex projects from start to finish \u2014 all while you focus on other priorities. Agents direct their own processes and tool usage, maintaining control over how they accomplish tasks with minimum human input. If you ask an agent to \"help plan my wedding\" it might autonomously research venues and vendors, compare pricing and availability, and create detailed timelines and budgets. Or if you ask it to \"prepare my company\u2019s board presentation\", it might search through your connected Google Drive for relevant sales reports and financial documents, extract key metrics from multiple spreadsheets, and produce a report. Last year, we introduced Claude Code , an agent that can autonomously write, debug, and edit code, and is used widely by software engineers. Many companies are also building their own agents using our models. Trellix, a cybersecurity firm, uses Claude to triage and investigate security issues. And Block , a financial services company, has built an agent that allows non-technical staff to access its data systems using natural language, saving its engineers time. Principles for trustworthy agents The rapid implementation of agents means it's crucial that developers like Anthropic build agents that are safe, reliable and trustworthy. Today, we're sharing an early framework for responsible agent development. We hope this framework can help establish emerging standards, offer adaptable guidance for different contexts, and contribute to building an ecosystem where agents align with human values. We aim to adhere to the following principles when developing agents: Keeping humans in control while enabling agent autonomy A central tension in agent design is balancing agent autonomy with human oversight. Agents must be able to work autonomously\u2014their independent operation is exactly what makes them valuable. But humans should retain control over how their goals are pursued, particularly before high-stakes decisions are made. For example, an agent helping with expense management might identify that the company is overspending on software subscriptions. Before it starts cancelling subscriptions or downgrading service tiers, the company would likely want a human to give approval. In Claude Code, humans can stop Claude whenever they want and redirect its approach. It has read-only permissions by default, meaning it can analyze and review information within the directory it's initialized in without asking for human approval, but must ask for human approval before taking any actions that modify code or systems. Users can grant persistent permissions for routine tasks they trust Claude to handle. As agents become more powerful and prevalent, we\u2019ll need even more robust technical solutions and intuitive user controls. The right balance between autonomy and oversight varies dramatically across scenarios and likely includes a mix of built-in and customizable oversight features. Transparency in agent behavior Humans need visibility into agents\u2019 problem-solving processes. Without transparency, a human asking an agent to \"reduce customer churn\" might be baffled when the agent starts contacting the facilities team about office layouts. But with good transparency design, the agent can explain its logic: \"I found that customers assigned to sales reps in the noisy open office area have 40% higher churn rates, so I'm requesting workspace noise assessments and proposing desk relocations to improve call quality.\" This also provides an opportunity to nudge agents in the right direction, by fact-checking their data, or making sure they\u2019re using",
        "chunk_index": 0,
        "start_char": 0,
        "end_char": 3945
      },
      {
        "id": "9ff11f78-308d-41dd-afc0-821a3b4631bd",
        "document_id": "6a29c5dd-3107-405f-bb1b-b1cee8575152",
        "content": "in agent behavior Humans need visibility into agents\u2019 problem-solving processes. Without transparency, a human asking an agent to \"reduce customer churn\" might be baffled when the agent starts contacting the facilities team about office layouts. But with good transparency design, the agent can explain its logic: \"I found that customers assigned to sales reps in the noisy open office area have 40% higher churn rates, so I'm requesting workspace noise assessments and proposing desk relocations to improve call quality.\" This also provides an opportunity to nudge agents in the right direction, by fact-checking their data, or making sure they\u2019re using the most relevant sources. In Claude Code, Claude shows its planned actions through a real-time to-do checklist, and users can jump in at any time to ask about or adjust Claude\u2019s workplan. The challenge is in finding the right level of detail. Too little information leaves humans unable to assess whether the agent is on track to achieve its goal. Too much can overwhelm them with irrelevant details. We try to take a middle ground but we\u2019ll need to iterate on this further. Claude Code\u2019s to-do checklist which users can see in real-time Aligning agents with human values and expectations Agents don't always act as humans intend. Our research has shown that when AI systems pursue goals autonomously, they can sometimes take actions that seem reasonable to the system but aren't what humans actually wanted. If a human asks an agent to \"organize my files,\" the agent might automatically delete what it considers duplicates and move files to new folder structures\u2014going far beyond simple organization to completely restructuring the user's system. While this stems from the agent trying to be helpful, it demonstrates how agents may lack the context to act appropriately even when their goals do align. More concerning are cases where agents pursue goals in ways that actively work against users' interests. Our testing of extreme scenarios has shown that when AI systems pursue goals autonomously, they can sometimes take action that seem reasonable to the system but violate what humans actually wanted. Users may also inadvertently prompt agents in ways that lead to unintended outcomes. Building reliable measures of agents\u2019 value alignment is challenging. It\u2019s hard to evaluate both the malign and benign causes of the problem at once. But we\u2019re actively figuring out how to resolve this problem. Until we have, the transparency and control principles above will be particularly important. Protecting privacy across extended interactions Agents can retain information across different tasks and interactions. This creates several potential privacy problems. Agents might inappropriately carry sensitive information from one context to another. For example, an agent might learn about confidential internal decisions from one department while helping with organizational planning, then inadvertently reference this information when assisting another department \u2013 exposing sensitive matters that should remain compartmentalized. Tools and processes that agents utilize should also be designed with the appropriate privacy protections and controls. The open-source Model Context Protocol (MCP) we created, which allows Claude to connect to other services, includes controls to enable users to allow or prevent Claude from accessing specific tools and processes, or what we call \u201cconnectors\u201d in a given task. In implementing MCP, we included additional controls, such as the option to grant one-time or permanent access to information. Enterprise administrators can also set which connectors users in their organizations can connect to. We continue to explore ways to improve our privacy protection tooling. We\u2019ve also outlined steps our customers should take to safeguard their data through measures like access permissions, authentication, and data segregation. Securing agents\u2019 interactions Agent systems should",
        "chunk_index": 1,
        "start_char": 3291,
        "end_char": 7262
      },
      {
        "id": "4a59ee7e-9686-45a8-9c54-d292dbcd17c5",
        "document_id": "6a29c5dd-3107-405f-bb1b-b1cee8575152",
        "content": "connect to other services, includes controls to enable users to allow or prevent Claude from accessing specific tools and processes, or what we call \u201cconnectors\u201d in a given task. In implementing MCP, we included additional controls, such as the option to grant one-time or permanent access to information. Enterprise administrators can also set which connectors users in their organizations can connect to. We continue to explore ways to improve our privacy protection tooling. We\u2019ve also outlined steps our customers should take to safeguard their data through measures like access permissions, authentication, and data segregation. Securing agents\u2019 interactions Agent systems should be designed to safeguard sensitive data and prevent misuse when interacting with other systems or agents. Since agents are tasked with achieving specific goals, attackers could trick an agent into ignoring its original instructions, revealing unauthorized information, or performing unintended actions by making it seem necessary to do so for the agent\u2019s objectives (also referred to as \"prompt injection\"). Or attackers could exploit vulnerabilities in the tools or sub-agents that agents use. Claude already uses a system of classifiers to detect and guard against misuses such as prompt injections, in addition to several other layers of security . Our Threat Intelligence team conducts ongoing monitoring to assess and mitigate new or emerging forms of malicious behaviour. In addition, we provide guidance on how organizations using Claude can further decrease these risks. Tools added to our Anthropic-reviewed MCP directory must adhere to our security, safety, and compatibility standards . When we discover new malicious behaviors or vulnerabilities through our monitoring and research, we strive to address them quickly and continuously improve our security measures to stay ahead of evolving threats. Next steps As we continue developing and improving our agents, we expect our understanding of their risks and trade-offs to also evolve. Over time, we\u2019ll plan to revise and update this framework to reflect our view of best practices. These principles will guide our current and future work on agent development, and we look forward to collaborating with other companies and organizations on this topic. Agents have tremendous potential for positive impacts in work, education, healthcare, and scientific discovery. That is why it is so important to ensure they are built to the highest standards. News Claude Opus 4.1 Aug 05, 2025 News Federal government departments and agencies can now purchase Claude through the GSA schedule Aug 05, 2025 News Anthropic Signs CMS Health Tech Ecosystem Pledge to Advance Healthcare Interoperability Jul 30, 2025",
        "chunk_index": 2,
        "start_char": 6578,
        "end_char": 9321
      }
    ],
    "created_at": "2025-08-06 09:15:51.030899",
    "file_size": 9634,
    "source_type": "web",
    "original_url": "https://www.anthropic.com/news/our-framework-for-developing-safe-and-trustworthy-agents"
  },
  "c3c1e20a-738d-41c9-9f5d-98a6cc32af1b": {
    "id": "c3c1e20a-738d-41c9-9f5d-98a6cc32af1b",
    "name": "AI as a Research Tool_ A Practical Guide.pdf",
    "file_path": "/Users/will/AVI BUILD/Able3_Main_WithVoiceMode/data/sources/AI_as_a_Research_Tool__A_Practical_Guide.pdf",
    "summary": "AI as a Research Tool: A Practical Guide A concise guide for using AI tools to enhance your research and content creation process Why AI Makes an Excellent Research Assistant AI tools excel at two critical research tasks that make them invaluable for creating high-quality content: Understanding Y...",
    "chunks": [
      {
        "id": "56854993-bb34-4fd5-8890-1603739ae91a",
        "document_id": "c3c1e20a-738d-41c9-9f5d-98a6cc32af1b",
        "content": "AI as a Research Tool: A Practical Guide A concise guide for using AI tools to enhance your research and content creation process Why AI Makes an Excellent Research Assistant AI tools excel at two critical research tasks that make them invaluable for creating high-quality content: Understanding Your Needs: AI can quickly assess your current knowledge level, identify your learning objectives, and tailor information to match your specific requirements. This personalised approach helps you get exactly the information you need at the right level of complexity. Processing Large Amounts of Information: AI tools are exceptional at taking vast amounts of data and transforming it into digestible, organised content. They can summarise complex research, identify key points, and present information in whatever format you need\u2014from bullet points to detailed reports. The Collaboration Approach Think of AI as your research assistant, not your replacement. Just as an academic might work with research assistants to help process large amounts of source material, you can collaborate with AI to handle the time-consuming aspects of research while you focus on the core concepts and creative thinking. This collaboration model means: \u25cf\u200b AI handles the heavy lifting of information processing \u25cf\u200b You provide the direction, context, and quality control \u25cf\u200b You remain in charge of the final output and fact-checking \u25cf\u200b The ideas and expertise are still fundamentally yours A Practical Workflow for AI-Assisted Research Step 1: Brain Dump Your Knowledge Before turning to AI, capture everything you already know about your topic. You can do this by: \u25cf\u200b Recording yourself speaking your thoughts aloud (using your phone's voice recorder) \u25cf\u200b Writing stream-of-consciousness notes \u25cf\u200b Creating a mind map of your existing knowledge This ensures your own expertise drives the process, with AI supporting rather than replacing your thinking. Step 2: Convert and Organise If you've recorded audio thoughts: \u25cf\u200b Use transcription tools (like Whisper or built-in transcription features) \u25cf\u200b Feed the transcript into your chosen AI tool \u25cf\u200b Ask the AI to organise and structure your raw thoughts Step 3: Enhance and Expand Once your initial thoughts are organised, use AI to: \u25cf\u200b Identify gaps in your research \u25cf\u200b Suggest additional areas to explore \u25cf\u200b Find supporting evidence for your points \u25cf\u200b Generate first drafts based on your structured thoughts Step 4: Quality Control Always review and fact-check AI output: \u25cf\u200b Verify factual claims, especially in specialised subjects \u25cf\u200b Check that sources are legitimate and current \u25cf\u200b Ensure the content matches your intended tone and style \u25cf\u200b Add your own insights and refinements Understanding AI Limitations The Hallucination Problem AI tools can sometimes generate plausible-sounding but incorrect information, particularly when dealing with: \u25cf\u200b Highly specialised or technical subjects \u25cf\u200b Recent events not in their training data \u25cf\u200b Niche topics with limited online sources How to Manage This Risk \u25cf\u200b Always fact-check important claims \u25cf\u200b Cross-reference information with reliable sources \u25cf\u200b Be especially cautious with technical, medical, or legal information \u25cf\u200b Use AI for structure and organisation rather than as your sole source of facts Recommended Tools for Different Tasks For Research and Information Gathering Perplexity: Excellent for research with real-time web access and source citations. Particularly good for current information and fact-checking. ChatGPT: Strong for processing and organising your existing knowledge, creating structured content, and generating ideas for further research. For Specialised Research Tasks STORM (by Stanford): Creates comprehensive, Wikipedia-style reports with citations. Useful for getting broad overviews of complex topics with proper source attribution. Practical Tips for Better Results Effective Prompting \u25cf\u200b Be specific about your audience and purpose \u25cf\u200b Provide context about your existing knowledge level \u25cf\u200b Ask for information to be presented",
        "chunk_index": 0,
        "start_char": 0,
        "end_char": 4029
      },
      {
        "id": "f7d8006b-a25a-4dc7-8ff7-77ee6f43082d",
        "document_id": "c3c1e20a-738d-41c9-9f5d-98a6cc32af1b",
        "content": "Tools for Different Tasks For Research and Information Gathering Perplexity: Excellent for research with real-time web access and source citations. Particularly good for current information and fact-checking. ChatGPT: Strong for processing and organising your existing knowledge, creating structured content, and generating ideas for further research. For Specialised Research Tasks STORM (by Stanford): Creates comprehensive, Wikipedia-style reports with citations. Useful for getting broad overviews of complex topics with proper source attribution. Practical Tips for Better Results Effective Prompting \u25cf\u200b Be specific about your audience and purpose \u25cf\u200b Provide context about your existing knowledge level \u25cf\u200b Ask for information to be presented in your preferred format \u25cf\u200b Request sources and citations when factual accuracy matters Removing Barriers in Your Process Identify the steps in your current research process that slow you down or create resistance: \u25cf\u200b Replace manual typing with voice-to-text where appropriate \u25cf\u200b Use AI to format and structure raw information \u25cf\u200b Automate repetitive tasks like citation formatting \u25cf\u200b Create templates for common research outputs Building Efficient Systems \u25cf\u200b Develop standard prompts for common research tasks \u25cf\u200b Create workflows that combine multiple tools effectively \u25cf\u200b Build databases of reliable sources for your field \u25cf\u200b Establish quality control checklists for AI-generated content Getting Started: Your First AI Research Project 1.\u200b Choose a topic you already know something about 2.\u200b Record or write your existing thoughts (10-15 minutes) 3.\u200b Feed this into an AI tool and ask it to create a structured outline 4.\u200b Identify gaps and ask the AI what additional areas you should research 5.\u200b Fact-check key claims using reliable sources or Perplexity 6.\u200b Refine and personalise the output with your own insights Key Takeaways \u25cf\u200b AI is a powerful research assistant, not a replacement for your expertise \u25cf\u200b Your knowledge and critical thinking remain essential to the process \u25cf\u200b Always verify important facts and claims \u25cf\u200b Focus on using AI to remove barriers and streamline workflows \u25cf\u200b The goal is to get from your ideas to finished output more efficiently \u25cf\u200b Quality control and human oversight are crucial for reliable results Remember The most effective use of AI in research maintains you in the driver's seat. You provide the expertise, direction, and quality control, while AI handles the time-consuming tasks of processing information and creating structured output. This collaboration allows you to focus on what humans do best: creative thinking, critical analysis, and adding unique insights to your work.",
        "chunk_index": 1,
        "start_char": 3283,
        "end_char": 5953
      }
    ],
    "created_at": "2025-08-10 09:30:23.119478",
    "file_size": 107890,
    "source_type": "pdf",
    "original_url": null
  }
}